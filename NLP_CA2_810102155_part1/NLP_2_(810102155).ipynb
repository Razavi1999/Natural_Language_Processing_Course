{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and PreProcessing  Data"
      ],
      "metadata": {
        "id": "9TQ1QhcE4ZC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzPbn9Ylsq1A",
        "outputId": "e4cc1465-ee26-46d0-9854-73344d593da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CgvaXpgs3Bt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP_CA2/training_1600000_processed_noemoticon.csv\"\n",
        "\n",
        "df = pd.read_csv(data_path , encoding = 'latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AabyrBn01A8I"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'0': 'target',\n",
        "                   '1467810369': 'ids' ,\n",
        "                   'Mon Apr 06 22:19:45 PDT 2009' : 'date' ,\n",
        "                   'NO_QUERY' : 'flag' ,\n",
        "                   '_TheSpecialOne_' : 'user' ,\n",
        "                   '@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D' : 'text'\n",
        "\n",
        "}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head().where(df[df['target'] == 4])\n",
        "\n",
        "# df['target' == 4]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "tiVsKqNDs_T-",
        "outputId": "bb8400e3-34b7-473a-d78f-40395acd1ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target         ids                          date      flag           user  \\\n",
              "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
              "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
              "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
              "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
              "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
              "\n",
              "                                                text  \n",
              "0  is upset that he can't update his Facebook by ...  \n",
              "1  @Kenichan I dived many times for the ball. Man...  \n",
              "2    my whole body feels itchy and like its on fire   \n",
              "3  @nationwideclass no, it's not behaving at all....  \n",
              "4                      @Kwesidei not the whole crew   "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb3fe0be-f598-4176-81b9-c0d42968b923\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811372</td>\n",
              "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joy_wolf</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb3fe0be-f598-4176-81b9-c0d42968b923')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb3fe0be-f598-4176-81b9-c0d42968b923 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb3fe0be-f598-4176-81b9-c0d42968b923');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e96da642-9fc1-4a43-b59c-36e6ea1b0cab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e96da642-9fc1-4a43-b59c-36e6ea1b0cab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e96da642-9fc1-4a43-b59c-36e6ea1b0cab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Th3G6CP17jt"
      },
      "source": [
        "**split dataset into partitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYVP8lxki1KN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "positive_samples = df[df['target'] == 4].sample(n=5000, random_state = 62)\n",
        "negative_samples = df[df['target'] == 0].sample(n=5000, random_state = 62)\n",
        "\n",
        "selected_samples = pd.concat([positive_samples, negative_samples])\n",
        "\n",
        "random_samples = selected_samples.sample(n = 2000 , random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csOuaFWX3F9Y"
      },
      "source": [
        "**tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "npiI-oF6QdYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwVXQ1c6h4n",
        "outputId": "f3e7799e-b4e3-4b14-c6aa-a5d3d2569d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2CCuFo03H9Z",
        "outputId": "f4de231e-650f-4aac-fbfc-16137fd2a733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Vocabulary: 104\n",
            "Length of Negative Vocabulary: 68\n",
            "Length of Positive Vocabulary: 99\n"
          ]
        }
      ],
      "source": [
        "def remove_punctuations(text):\n",
        "  # initializing punctuations string\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "  for ele in text:\n",
        "      if ele in punc:\n",
        "          text = text.replace(ele  ,  \"\")\n",
        "  return text\n",
        "\n",
        "def tokenize(text):\n",
        "    words = text.lower().split()\n",
        "    words = remove_punctuations(text)\n",
        "    stems = stemmer.stem(text)\n",
        "    filtered_words = [word for word in stems if word not in stopwords.words('english')]\n",
        "    return set(filtered_words)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "vocabulary = set()\n",
        "vocab_positive = set()\n",
        "vocab_negative = set()\n",
        "\n",
        "\n",
        "for index , row in selected_samples.iterrows() :\n",
        "    # print(f'original text             : {text}')\n",
        "    # print(item)\n",
        "    text = row['text']\n",
        "    target = row['target']\n",
        "\n",
        "    text = remove_punctuations(text)\n",
        "\n",
        "    # print(f'text without punctuations : {text}')\n",
        "\n",
        "    stems = stemmer.stem(text)\n",
        "    # print(f'stems                     : {stems}')\n",
        "    # print('------------------------------------')\n",
        "\n",
        "    if target == 0:\n",
        "      vocab_negative.update(tokenize(stems))\n",
        "\n",
        "    else:\n",
        "      vocab_positive.update(tokenize(stems))\n",
        "\n",
        "    vocabulary.update(tokenize(stems))\n",
        "\n",
        "print(\"Length of Vocabulary:\", len(vocabulary))\n",
        "print(\"Length of Negative Vocabulary:\", len(vocab_negative))\n",
        "print(\"Length of Positive Vocabulary:\", len(vocab_positive))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build Document Term Matrix**"
      ],
      "metadata": {
        "id": "KFDGmYWzvxv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxC2Ler4t0o"
      },
      "outputs": [],
      "source": [
        "def build_document_term_matrix(data , vocabulary):\n",
        "    matrix = []\n",
        "    for index , row in data.iterrows():\n",
        "        ids = row['ids']\n",
        "        text = row['text']\n",
        "\n",
        "        word_count = {word : 0 for word in vocabulary}\n",
        "        tokens = tokenize(text)\n",
        "\n",
        "        for token in tokens :\n",
        "          if token in vocabulary:\n",
        "            word_count[token] += 1\n",
        "\n",
        "        matrix.append(list(word_count.values()))\n",
        "\n",
        "    return matrix\n",
        "\n",
        "# Build document-term matrix\n",
        "document_term_negative = build_document_term_matrix(negative_samples , vocab_negative)\n",
        "document_term_positive = build_document_term_matrix(positive_samples , vocab_positive)\n",
        "\n",
        "# print(f'length of document_term_negative : {len(document_term_negative)}')\n",
        "# print(f'length of document_term_positive : {len(document_term_positive)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**naive bayes (doucment term matrix)**"
      ],
      "metadata": {
        "id": "R4UUdbztlsg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class naive_bayes_1:\n",
        "  def __init__(self , document_term_negative , document_term_positive , vocab_positive , vocab_negative):\n",
        "    self.prior = {}\n",
        "    self.likelihood = {}\n",
        "\n",
        "    self.positives = document_term_positive\n",
        "    self.negatives = document_term_negative\n",
        "\n",
        "    self.vocab_positive = vocab_positive\n",
        "    self.vocab_negative = vocab_negative\n",
        "\n",
        "\n",
        "  def fit(self , X_train , y_train):\n",
        "    classes , counts = np.unique(y_train , return_counts = True)\n",
        "\n",
        "    total_samples = len(y_train)\n",
        "    for c, count in zip(classes, counts):\n",
        "        self.prior[c] = count / total_samples\n",
        "\n",
        "    unq_words_count_positive = len(vocab_positive)\n",
        "    unq_words_count_negative = len(vocab_negative)\n",
        "\n",
        "    matrix  = np.array(self.positives)\n",
        "    matrix2 = np.array(self.negatives)\n",
        "\n",
        "    all_words_count_positive = np.sum(matrix)\n",
        "    all_words_count_negative = np.sum(matrix2)\n",
        "\n",
        "    # print(f'shape of matrix {matrix.shape}')\n",
        "    # print(f'length of positives : {len(self.positives)}')\n",
        "\n",
        "    # for i in range(0 , len(self.positives)):\n",
        "    #   all_words_count_positive += np.sum(matrix[: , i])\n",
        "\n",
        "    # for i in range(0 , len(self.negatives)):\n",
        "    #   all_words_count_negative += np.sum(matrix[: , i])\n",
        "\n",
        "    # matrix = np.array(self.positives)\n",
        "\n",
        "    self.likelihood[4] = {}\n",
        "    self.likelihood[0] = {}\n",
        "\n",
        "    index = 0\n",
        "    for word in self.vocab_positive:\n",
        "      column_sum = np.sum(matrix[:, index])\n",
        "      self.likelihood[4][word] = (1 + column_sum) / (all_words_count_positive + unq_words_count_positive)\n",
        "      index += 1\n",
        "\n",
        "\n",
        "    index = 0\n",
        "    for word in self.vocab_negative:\n",
        "      column_sum = np.sum(matrix2[:, index])\n",
        "      self.likelihood[0][word] = (1 + column_sum) / (all_words_count_negative + unq_words_count_negative)\n",
        "      index += 1\n",
        "\n",
        "\n",
        "  def predict(self , X_test):\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    df = pd.DataFrame(X_test , columns = ['ids', 'date', 'flag' , 'user' , 'text'])\n",
        "    df.head()\n",
        "\n",
        "    predicted_labels = []\n",
        "\n",
        "    for index , row in df.iterrows():\n",
        "      class_scores = {}\n",
        "      class_scores[0] = np.log(self.prior[0])\n",
        "      class_scores[4] = np.log(self.prior[4])\n",
        "\n",
        "      # print(f'row.columns : {row.columns}' )\n",
        "      # ids  = row['ids']\n",
        "      text = row['text']\n",
        "      text = tokenize(text)\n",
        "\n",
        "      for word in text:\n",
        "        if word in vocab_negative:\n",
        "              class_scores[0] += np.log(self.likelihood[0][word])\n",
        "\n",
        "        if word in vocab_positive:\n",
        "              class_scores[4] += np.log(self.likelihood[4][word])\n",
        "\n",
        "      predicted_labels.append(0 if class_scores[0] > class_scores[4] else 4)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "  def calculate_metrics(self , y_pred, true_labels):\n",
        "    true_positives = sum((pred == 4 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "    false_positives = sum((pred == 4 and true == 0) for pred, true in zip(y_pred, true_labels))\n",
        "    false_negatives = sum((pred == 0 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "jj_wNoh0lxhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Naive Bayes Model**"
      ],
      "metadata": {
        "id": "zG2ysg1XTlbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "random_samples = selected_samples.sample(n = 2000 , random_state = 42)\n",
        "\n",
        "# Split data into features and labels\n",
        "X = selected_samples.drop('target', axis=1).values\n",
        "y = selected_samples['target'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "nb = naive_bayes_1(document_term_negative , document_term_positive , vocab_positive , vocab_negative)\n",
        "nb.fit(X_train , y_train)\n",
        "\n",
        "predicted_labels = nb.predict(X_test)\n",
        "true_labels = y_test\n",
        "\n",
        "precision, recall, f1_score = nb.calculate_metrics(predicted_labels , true_labels)\n",
        "print(f'precision : {precision}')\n",
        "print(f'recall : {recall}')\n",
        "print(f'f1_score : {f1_score}')"
      ],
      "metadata": {
        "id": "HAZTJQThxA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65b670f-6e0c-4b5c-de33-8a193086644b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision : 0.6042345276872965\n",
            "recall : 0.366600790513834\n",
            "f1_score : 0.45633456334563344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC01aaK62PX_"
      },
      "source": [
        "# **calculate tf-idf**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**calculate TF-IDF**"
      ],
      "metadata": {
        "id": "AuKO_AY6xSY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-XVsyJjivN2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def calculate_tfidf(matrix):\n",
        "    num_documents = len(matrix)\n",
        "    word_count_per_document = np.sum(np.array(matrix) > 0, axis=0)\n",
        "\n",
        "    idf_values = {j: math.log(num_documents / count) if count != 0 else 0 for j, count in enumerate(word_count_per_document)}\n",
        "\n",
        "    tfidf_matrix = np.zeros_like(matrix, dtype=float)\n",
        "    for i, row in enumerate(matrix):\n",
        "        for j, count in enumerate(row):\n",
        "            if count > 0:\n",
        "                tf = count / np.sum(row)\n",
        "                tfidf_matrix[i, j] = tf * idf_values.get(j, 0)\n",
        "\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate TF-IDF values\n",
        "tfidf_matrix_negative = calculate_tfidf(document_term_negative)\n",
        "tfidf_matrix_positive = calculate_tfidf(document_term_positive)\n",
        "\n",
        "print(f'length of tfidf_matrix_positive : {len(tfidf_matrix_positive)}')\n",
        "print(f'length of tfidf_matrix_negative : {len(tfidf_matrix_negative)}')\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "# for row in tfidf_matrix:\n",
        "#     print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**naive bayes with TF-IDF Matrix**"
      ],
      "metadata": {
        "id": "N2IcTe1qxmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class naive_bayes_2:\n",
        "\n",
        "  def __init__(self , document_term_negative , document_term_positive , vocab_positive , vocab_negative):\n",
        "    self.prior = {}\n",
        "    self.likelihood = {}\n",
        "\n",
        "    self.positives = document_term_positive\n",
        "    self.negatives = document_term_negative\n",
        "\n",
        "    self.vocab_positive = vocab_positive\n",
        "    self.vocab_negative = vocab_negative\n",
        "\n",
        "\n",
        "  def fit(self , X_train , y_train):\n",
        "    classes , counts = np.unique(y_train , return_counts = True)\n",
        "\n",
        "    total_samples = len(y_train)\n",
        "    for c, count in zip(classes, counts):\n",
        "        self.prior[c] = count / total_samples\n",
        "\n",
        "    unq_words_count_positive = len(vocab_positive)\n",
        "    unq_words_count_negative = len(vocab_negative)\n",
        "\n",
        "\n",
        "    all_words_count_positive = 0\n",
        "    all_words_count_negative = 0\n",
        "\n",
        "    matrix  = np.array(self.positives)\n",
        "    matrix2 = np.array(self.negatives)\n",
        "\n",
        "    all_words_tfidf_positive = np.sum(matrix)\n",
        "    all_words_tfidf_negative = np.sum(matrix2)\n",
        "\n",
        "    # for i in range(len(self.positives)):\n",
        "    #   all_words_count_positive += np.sum(matrix[: , i])\n",
        "\n",
        "    # for i in range(len(self.negatives)):\n",
        "    #   all_words_count_negative += np.sum(matrix[: , i])\n",
        "\n",
        "    # matrix = np.array(self.positives)\n",
        "\n",
        "    self.likelihood[4] = {}\n",
        "    self.likelihood[0] = {}\n",
        "\n",
        "    index = 0\n",
        "    for word in self.vocab_positive:\n",
        "      column_sum = np.sum(matrix[:, index])\n",
        "      self.likelihood[4][word] = (1 + column_sum) / (all_words_tfidf_positive + unq_words_count_positive)\n",
        "      index += 1\n",
        "\n",
        "\n",
        "    index = 0\n",
        "    for word in self.vocab_negative:\n",
        "      column_sum = np.sum(matrix2[:, index])\n",
        "      self.likelihood[0][word] = (1 + column_sum) / (all_words_tfidf_negative + unq_words_count_negative)\n",
        "      index += 1\n",
        "\n",
        "\n",
        "  def predict(self , X_test):\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    df = pd.DataFrame(X_test , columns = ['ids', 'date', 'flag' , 'user' , 'text'])\n",
        "    df.head()\n",
        "\n",
        "    predicted_labels = []# print(f'length of document_matrix : {len(document_term_matrix)}')\n",
        "# Print the document-term matrix\n",
        "# for row in document_term_matrix:\n",
        "#     print(row)\n",
        "\n",
        "    for index , row in df.iterrows():\n",
        "      class_scores = {}\n",
        "      class_scores[0] = np.log(self.prior[0])\n",
        "      class_scores[4] = np.log(self.prior[4])\n",
        "\n",
        "      # print(f'row.columns : {row.columns}' )\n",
        "      # ids  = row['ids']\n",
        "      text = row['text']\n",
        "      text = tokenize(text)\n",
        "\n",
        "      for word in text:\n",
        "        if word in vocab_negative:\n",
        "              class_scores[0] += np.log(self.likelihood[0][word])\n",
        "\n",
        "        if word in vocab_positive:\n",
        "              class_scores[4] += np.log(self.likelihood[4][word])\n",
        "\n",
        "      predicted_labels.append(0 if class_scores[0] > class_scores[4] else 4)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "  def calculate_metrics(self , y_pred, true_labels):\n",
        "    true_positives = sum((pred == 4 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "    false_positives = sum((pred == 4 and true == 0) for pred, true in zip(y_pred, true_labels))\n",
        "    false_negatives = sum((pred == 0 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "ZIEH5j_DxcmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using naive bayes with TF-IDF Matrix**"
      ],
      "metadata": {
        "id": "xL07x7Mqx_V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "random_samples = selected_samples.sample(n = 2000 , random_state = 42)\n",
        "\n",
        "# Split data into features and labels\n",
        "X = selected_samples.drop('target', axis=1).values\n",
        "y = selected_samples['target'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "nb = naive_bayes_1(tfidf_matrix_negative , tfidf_matrix_positive , vocab_positive , vocab_negative)\n",
        "nb.fit(X_train , y_train)\n",
        "\n",
        "predicted_labels = nb.predict(X_test)\n",
        "true_labels = y_test\n",
        "\n",
        "precision, recall, f1_score = nb.calculate_metrics(predicted_labels , true_labels)\n",
        "print(f'precision : {precision}')\n",
        "print(f'recall : {recall}')\n",
        "print(f'f1_score : {f1_score}')"
      ],
      "metadata": {
        "id": "T17f3eT6yHhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8acad5f7-7f17-4376-cfa3-39d0a6b1d8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision : 0.7142857142857143\n",
            "recall : 0.004940711462450593\n",
            "f1_score : 0.009813542688910697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-s7TgA8ShW"
      },
      "source": [
        "# **ppmi matrix**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_ppmi_matrix(df , vocabulary):\n",
        "  co_occurence = {}\n",
        "  vocab_size = len(vocabulary)\n",
        "  vocabulary = list(vocabulary)\n",
        "\n",
        "  co_occurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    tokenized = tokenize(row['text'])\n",
        "\n",
        "    for x in tokenized:\n",
        "      for y in tokenized:\n",
        "        if x in vocabulary and y in vocabulary:\n",
        "          i = vocabulary.index(x)\n",
        "          j = vocabulary.index(y)\n",
        "\n",
        "        if x == y:\n",
        "          if x not in co_occurence.keys():\n",
        "            co_occurence[x] = 1\n",
        "            co_occurrence_matrix[i][i] = 1\n",
        "\n",
        "          else:\n",
        "            co_occurence[x] = co_occurence[x] + 1\n",
        "            co_occurrence_matrix[i][i] = co_occurrence_matrix[i][i] + 1\n",
        "\n",
        "          continue\n",
        "\n",
        "        # print(x , y)\n",
        "\n",
        "        key  = (x , y)\n",
        "        key2 = (y , x)\n",
        "\n",
        "        if key not in co_occurence.keys():\n",
        "          co_occurence[key] = 1\n",
        "          co_occurence[key2] = 1\n",
        "          co_occurrence_matrix[i][j] = 1\n",
        "          co_occurrence_matrix[j][i] = 1\n",
        "\n",
        "\n",
        "        else:\n",
        "          co_occurence[key] = co_occurence[key] + 1\n",
        "          co_occurence[key2] = co_occurence[key2] + 1\n",
        "          co_occurrence_matrix[i][j] = co_occurrence_matrix[i][j] + 1\n",
        "          co_occurrence_matrix[j][i] = co_occurrence_matrix[j][i] + 1\n",
        "\n",
        "\n",
        "\n",
        "  co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
        "\n",
        "  # Calculate probabilities\n",
        "  word_counts = np.sum(co_occurrence_matrix, axis=1)\n",
        "  total_word_count = np.sum(word_counts)\n",
        "  P_A = word_counts / total_word_count\n",
        "  P_B = word_counts / total_word_count\n",
        "  P_A_B = co_occurrence_matrix / total_word_count\n",
        "\n",
        "  # Calculate PMI\n",
        "  PMI_matrix = np.log(P_A_B / (P_A[:, None] * P_B))\n",
        "\n",
        "  # Calculate PPMI\n",
        "  PPMI_matrix = np.maximum(PMI_matrix, 0)\n",
        "\n",
        "  # PPMI_matrix = np.nan_to_num(PPMI_matrix)\n",
        "\n",
        "  return PPMI_matrix\n",
        "\n",
        "ppmi_matrix_negative = build_ppmi_matrix(negative_samples , vocab_negative)\n",
        "ppmi_matrix_positive = build_ppmi_matrix(positive_samples , vocab_positive)\n",
        "\n",
        "print(f'ppmi_matrix_negative : {ppmi_matrix_negative[0 : 100]}')"
      ],
      "metadata": {
        "id": "QPDJmWuwbkfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e86cfa-53b4-4dca-c759-e51435ac9b94"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-9268ee52deae>:60: RuntimeWarning: divide by zero encountered in log\n",
            "  PMI_matrix = np.log(P_A_B / (P_A[:, None] * P_B))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ppmi_matrix_negative : [[7.57613742 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         4.77498898 0.         ... 4.93082885 5.2847603  0.74427915]\n",
            " [0.         0.         0.26339893 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         4.93082885 0.         ... 6.3786524  0.         1.58119362]\n",
            " [0.         5.2847603  0.         ... 0.         6.28003945 0.60012401]\n",
            " [0.         0.74427915 0.         ... 1.58119362 0.60012401 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aCI_ydNicpY"
      },
      "source": [
        "**Naive Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class naive_bayes_3:\n",
        "  def __init__(self , document_term_negative , document_term_positive , vocab_positive , vocab_negative):\n",
        "    self.prior = {}\n",
        "    self.likelihood = {}\n",
        "\n",
        "    self.positives = document_term_positive\n",
        "    self.negatives = document_term_negative\n",
        "\n",
        "    self.vocab_positive = list(vocab_positive)\n",
        "    self.vocab_negative = list(vocab_negative)\n",
        "\n",
        "\n",
        "  def fit(self , X_train , y_train):\n",
        "    classes , counts = np.unique(y_train , return_counts = True)\n",
        "\n",
        "    total_samples = len(y_train)\n",
        "    for c, count in zip(classes, counts):\n",
        "        self.prior[c] = count / total_samples\n",
        "\n",
        "    unq_words_count_positive = len(vocab_positive)\n",
        "    unq_words_count_negative = len(vocab_negative)\n",
        "\n",
        "    matrix  = np.array(self.positives)\n",
        "    matrix2 = np.array(self.negatives)\n",
        "\n",
        "    all_words_ppmi_positive = np.sum(matrix)\n",
        "    all_words_ppmi_negative = np.sum(matrix2)\n",
        "\n",
        "    # self.likelihood[4] = {}\n",
        "    # self.likelihood[0] = {}\n",
        "\n",
        "    # index = 0\n",
        "    # for word in self.vocab_positive:\n",
        "    #   column_sum = np.sum(matrix[:, index])\n",
        "    #   # print(f'column_sum : {column_sum}')\n",
        "    #   self.likelihood[4][word] = (1 + column_sum) / (all_words_ppmi_positive + unq_words_count_positive)\n",
        "    #   index += 1\n",
        "\n",
        "\n",
        "    # index = 0\n",
        "    # for word in self.vocab_negative:\n",
        "    #   column_sum = np.sum(matrix2[:, index])\n",
        "    #   # print(f'column_sum : {column_sum}')\n",
        "    #   self.likelihood[0][word] = (1 + column_sum) / (all_words_ppmi_negative + unq_words_count_negative)\n",
        "    #   index += 1\n",
        "\n",
        "\n",
        "  def predict(self , X_test):\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    df = pd.DataFrame(X_test , columns = ['ids', 'date', 'flag' , 'user' , 'text'])\n",
        "    df.head()\n",
        "\n",
        "    predicted_labels = []\n",
        "\n",
        "    for index , row in df.iterrows():\n",
        "      class_scores = {}\n",
        "      class_scores[0] = np.log(self.prior[0])\n",
        "      class_scores[4] = np.log(self.prior[4])\n",
        "\n",
        "      text = row['text']\n",
        "      text = tokenize(text)\n",
        "\n",
        "      length_of_sentence = len(text)\n",
        "\n",
        "      desired_positive = np.zeros(len(self.vocab_positive))\n",
        "      desired_negative = np.zeros(len(self.vocab_negative))\n",
        "\n",
        "      inedexes_positive = []\n",
        "      indexes_negative  = []\n",
        "\n",
        "      for word in text:\n",
        "        if word in vocab_negative:\n",
        "              index = self.vocab_negative.index(word)\n",
        "              indexes_negative.append(index)\n",
        "              desired_negative = desired_negative + self.negatives[index]\n",
        "              # class_scores[0] += np.log(self.likelihood[0][word])\n",
        "\n",
        "        if word in vocab_positive:\n",
        "              index = self.vocab_positive.index(word)\n",
        "              inedexes_positive.append(index)\n",
        "              desired_positive = desired_positive + self.positives[index]\n",
        "              # class_scores[4] += np.log(self.likelihood[4][word])\n",
        "\n",
        "      desired_negative = desired_negative / length_of_sentence\n",
        "      desired_positive = desired_positive / length_of_sentence\n",
        "\n",
        "      class_scores[0] += sum(np.log(x) if index in indexes_negative else 0 for index , x in enumerate(desired_negative))\n",
        "      class_scores[4] += sum(np.log(x) if index in inedexes_positive else 0 for index , x in enumerate(desired_positive))\n",
        "\n",
        "\n",
        "      predicted_labels.append(0 if class_scores[0] > class_scores[4] else 4)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "  def calculate_metrics(self , y_pred, true_labels):\n",
        "    true_positives = sum((pred == 4 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "    false_positives = sum((pred == 4 and true == 0) for pred, true in zip(y_pred, true_labels))\n",
        "    false_negatives = sum((pred == 0 and true == 4) for pred, true in zip(y_pred, true_labels))\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "kbZG0_5IcUAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into features and labels\n",
        "X = selected_samples.drop('target', axis=1).values\n",
        "y = selected_samples['target'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 52)\n",
        "\n",
        "\n",
        "nb = naive_bayes_3(ppmi_matrix_negative , ppmi_matrix_positive , vocab_positive , vocab_negative)\n",
        "nb.fit(X_train , y_train)\n",
        "\n",
        "predicted_labels = nb.predict(X_test)\n",
        "true_labels = y_test\n",
        "\n",
        "precision, recall, f1_score = nb.calculate_metrics(predicted_labels , true_labels)\n",
        "print(f'precision : {precision}')\n",
        "print(f'recall : {recall}')\n",
        "print(f'f1_score : {f1_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6VfwgZPcx99",
        "outputId": "b46eb41e-84d6-4ba5-f7c3-cd4fa6c893ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-d26e2cbcdac2>:91: RuntimeWarning: divide by zero encountered in log\n",
            "  class_scores[4] += sum(np.log(x) if index in inedexes_positive else 0 for index , x in enumerate(desired_positive))\n",
            "<ipython-input-16-d26e2cbcdac2>:90: RuntimeWarning: divide by zero encountered in log\n",
            "  class_scores[0] += sum(np.log(x) if index in indexes_negative else 0 for index , x in enumerate(desired_negative))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision : 0.46464646464646464\n",
            "recall : 0.046890927624872576\n",
            "f1_score : 0.08518518518518518\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}