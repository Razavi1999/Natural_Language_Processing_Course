<!DOCTYPE html>
<!-- saved from url=(0128)https://medium.com/@bloomfountaincoder/fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9 -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="origin-trial" content="Az520Inasey3TAyqLyojQa8MnmCALSEU29yQFW8dePZ7xQTvSt73pHazLFTK5f7SyLUJSo2uKLesEtEa9aUYcgMAAACPeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZS5jb206NDQzIiwiZmVhdHVyZSI6IkRpc2FibGVUaGlyZFBhcnR5U3RvcmFnZVBhcnRpdGlvbmluZyIsImV4cGlyeSI6MTcyNTQwNzk5OSwiaXNTdWJkb21haW4iOnRydWUsImlzVGhpcmRQYXJ0eSI6dHJ1ZX0="><title>Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA. | by Hidiat Ibrahim | Medium</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-03-13T05:13:17.175Z"><meta data-rh="true" name="title" content="Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA. | by Hidiat Ibrahim | Medium"><meta data-rh="true" property="og:title" content="Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA."><meta data-rh="true" property="al:android:url" content="medium://p/388dcfb1c7e9"><meta data-rh="true" property="al:ios:url" content="medium://p/388dcfb1c7e9"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Large language models (LLMs) typically have huge parameters running from hundreds of millions to billions due to the huge datasets they were trained on and several layers of their transformer model…"><meta data-rh="true" property="og:description" content="Fine-tune Falcon 7B LLM."><meta data-rh="true" property="og:url" content="https://medium.com/@bloomfountaincoder/fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9"><meta data-rh="true" property="al:web:url" content="https://medium.com/@bloomfountaincoder/fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9"><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/da:true/resize:fit:1200/0*9PSSJMGe60zzIkKE"><meta data-rh="true" property="article:author" content="https://medium.com/@bloomfountaincoder"><meta data-rh="true" name="author" content="Hidiat Ibrahim"><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" property="twitter:title" content="Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA."><meta data-rh="true" name="twitter:site" content="@Medium"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/388dcfb1c7e9"><meta data-rh="true" property="twitter:description" content="Fine-tune Falcon 7B LLM."><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/da:true/resize:fit:1200/0*9PSSJMGe60zzIkKE"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="twitter:label1" content="Reading time"><meta data-rh="true" name="twitter:data1" content="10 min read"><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" rel="preconnect" href="https://glyph.medium.com/" crossorigin=""><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="./Tutorial_files/unbound.css"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Tutorial_files/unbound.css"><link data-rh="true" rel="author" href="https://medium.com/@bloomfountaincoder"><link data-rh="true" rel="canonical" href="https://medium.com/@bloomfountaincoder/fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/388dcfb1c7e9"><style type="text/css" data-fela-rehydration="526" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="526" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.ds{margin-left:8px}.dt{color:#6B6B6B}.du{font-size:13px}.dv{height:100%}.eo{color:#FFFFFF}.ep{fill:#FFFFFF}.eq{background:#1A8917}.er{border-color:#1A8917}.ev:disabled{cursor:inherit !important}.ew:disabled{opacity:0.3}.ex:disabled:hover{background:#1A8917}.ey:disabled:hover{border-color:#1A8917}.ez{border-radius:99em}.fa{border-width:1px}.fb{border-style:solid}.fc{box-sizing:border-box}.fd{text-decoration:none}.fe{text-align:center}.fh{margin-right:32px}.fi{position:relative}.fj{fill:#6B6B6B}.fm{background:transparent}.fn svg{margin-left:4px}.fo svg{fill:#6B6B6B}.fq{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fr{position:absolute}.fy{margin:0 24px}.gc{background:rgba(255, 255, 255, 1)}.gd{border:1px solid #F2F2F2}.ge{box-shadow:0 1px 4px #F2F2F2}.gf{max-height:100vh}.gg{overflow-y:auto}.gh{left:0}.gi{top:calc(100vh + 100px)}.gj{bottom:calc(100vh + 100px)}.gk{width:10px}.gl{pointer-events:none}.gm{word-break:break-word}.gn{word-wrap:break-word}.go:after{display:block}.gp:after{content:""}.gq:after{clear:both}.gr{line-height:1.23}.gs{letter-spacing:0}.gt{font-style:normal}.gu{font-weight:700}.hu{@media all and (max-width: 551.98px):8px}.hv{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.hw{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.hx{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.hy{@media all and (min-width: 1080px):16px}.ie{align-items:baseline}.if{width:48px}.ig{height:48px}.ih{border:2px solid rgba(255, 255, 255, 1)}.ii{z-index:0}.ij{box-shadow:none}.ik{border:1px solid rgba(0, 0, 0, 0.05)}.il{margin-bottom:2px}.im{flex-wrap:nowrap}.in{font-size:16px}.io{line-height:24px}.iq{margin:0 8px}.ir{display:inline}.is{color:#1A8917}.it{fill:#1A8917}.iw{flex:0 0 auto}.iz{flex-wrap:wrap}.ja{padding-left:8px}.jb{padding-right:8px}.kc> *{flex-shrink:0}.kd{overflow-x:scroll}.ke::-webkit-scrollbar{display:none}.kf{scrollbar-width:none}.kg{-ms-overflow-style:none}.kh{width:74px}.ki{flex-direction:row}.kj{z-index:2}.kk{margin-right:4px}.kn{-webkit-user-select:none}.ko{border:0}.kp{fill:rgba(117, 117, 117, 1)}.ks{outline:0}.kt{user-select:none}.ku> svg{pointer-events:none}.ld{cursor:progress}.le{margin-left:4px}.lf{margin-top:0px}.lg{opacity:1}.lh{padding:4px 0}.lk{width:16px}.lm{display:inline-flex}.ls{max-width:100%}.lt{padding:8px 2px}.lu svg{color:#6B6B6B}.ml{line-height:1.12}.mm{letter-spacing:-0.022em}.mn{font-weight:600}.ni{margin-bottom:-0.28em}.nj{margin-left:auto}.nk{margin-right:auto}.nl{max-width:2947px}.nr{clear:both}.nt{cursor:zoom-in}.nu{z-index:auto}.nw{height:auto}.nx{margin-top:10px}.ny{max-width:728px}.ob{text-decoration:underline}.oc{line-height:1.58}.od{letter-spacing:-0.004em}.oe{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.oz{margin-bottom:-0.46em}.pa{list-style-type:decimal}.pb{margin-left:30px}.pc{padding-left:0px}.pi{line-height:1.18}.pw{margin-bottom:-0.31em}.qc{overflow-x:auto}.qd{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.qe{padding:32px}.qf{border:1px solid #E5E5E5}.qg{line-height:1.4}.qh{margin-top:-0.2em}.qi{margin-bottom:-0.2em}.qj{white-space:pre}.qk{min-width:fit-content}.ql{margin-top:16px}.qm{max-width:894px}.qn{max-width:1498px}.qo{font-style:italic}.qp{max-width:871px}.qq{max-width:1597px}.qr{list-style-type:disc}.qs{margin-bottom:26px}.qt{margin-top:6px}.qu{margin-top:8px}.qv{margin-right:8px}.qw{padding:8px 16px}.qx{border-radius:100px}.qy{transition:background 300ms ease}.ra{white-space:nowrap}.rb{border-top:none}.rh{height:52px}.ri{max-height:52px}.rj{box-sizing:content-box}.rk{position:static}.rl{z-index:1}.rn{max-width:155px}.rt{margin-right:20px}.rz{align-items:flex-end}.sa{width:76px}.sb{height:76px}.sc{border:2px solid #F9F9F9}.sd{height:72px}.se{width:72px}.sf{width:auto}.sg{stroke:#F2F2F2}.sh{height:36px}.si{width:36px}.sj{color:#F2F2F2}.sk{fill:#F2F2F2}.sl{background:#F2F2F2}.sm{border-color:#F2F2F2}.ss{font-weight:500}.st{font-size:24px}.su{line-height:30px}.sv{letter-spacing:-0.016em}.sw{height:0px}.sx{border-bottom:solid 1px #E5E5E5}.td{margin-top:72px}.te{padding:24px 0}.tf{margin-bottom:0px}.tg{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.es:hover{background:#156D12}.et:hover{border-color:#156D12}.eu:hover{cursor:pointer}.fk:hover{color:#242424}.fl:hover{fill:#242424}.fp:hover svg{fill:#242424}.fs:hover{background-color:rgba(0, 0, 0, 0.1)}.ip:hover{text-decoration:underline}.iu:hover:not(:disabled){color:#156D12}.iv:hover:not(:disabled){fill:#156D12}.kr:hover{fill:rgba(8, 8, 8, 1)}.li:hover{fill:#000000}.lj:hover p{color:#000000}.ll:hover{color:#000000}.lv:hover svg{color:#000000}.qz:hover{background-color:#F2F2F2}.sn:hover{background:#F2F2F2}.so:hover{border-color:#F2F2F2}.sp:hover{cursor:wait}.sq:hover{color:#F2F2F2}.sr:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.kq:focus{fill:rgba(8, 8, 8, 1)}.lw:focus svg{color:#000000}.nv:focus{transform:scale(1.01)}.kv:active{border-style:none}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ee{font-size:14px}.ef{line-height:20px}.el{font-size:13px}.em{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.hp{font-size:42px}.hq{margin-top:1.19em}.hr{margin-bottom:32px}.hs{line-height:52px}.ht{letter-spacing:-0.011em}.id{align-items:center}.jo{border-top:solid 1px #F2F2F2}.jp{border-bottom:solid 1px #F2F2F2}.jq{margin:32px 0 0}.jr{padding:3px 8px}.ka> *{margin-right:24px}.kb> :last-child{margin-right:0}.lc{margin-top:0px}.lr{margin:0}.ne{font-size:24px}.nf{margin-top:1.95em}.ng{line-height:30px}.nh{letter-spacing:-0.016em}.nq{margin-top:56px}.ov{font-size:20px}.ow{margin-top:2.14em}.ox{line-height:32px}.oy{letter-spacing:-0.003em}.ph{margin-top:1.14em}.pt{margin-top:1.72em}.pu{line-height:24px}.pv{letter-spacing:0}.qb{margin-top:0.94em}.rg{margin-bottom:88px}.rs{display:inline-block}.ry{padding-top:72px}.tc{margin-top:40px}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.lb{margin-top:0px}.nz{margin-left:auto}.oa{text-align:center}.rr{display:inline-block}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.la{margin-top:0px}.rq{display:inline-block}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ky{margin-top:0px}.kz{margin-right:0px}.rp{display:inline-block}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.dw{font-size:13px}.dx{line-height:20px}.eg{padding:0px 8px 1px}.ft{margin-bottom:4px}.gv{font-size:32px}.gw{margin-top:1.01em}.gx{margin-bottom:24px}.gy{line-height:38px}.gz{letter-spacing:-0.014em}.hz{align-items:flex-start}.ix{flex-direction:column}.jc{margin:24px -24px 0}.jd{padding:0}.js> *{margin-right:8px}.jt> :last-child{margin-right:24px}.kl{margin-left:0px}.kw{margin-top:0px}.kx{margin-right:0px}.ln{margin:0}.lx{border:1px solid #F2F2F2}.ly{border-radius:99em}.lz{padding:0px 16px 0px 12px}.ma{height:38px}.mb{align-items:center}.md svg{margin-right:8px}.mo{font-size:20px}.mp{margin-top:1.2em}.mq{line-height:24px}.mr{letter-spacing:0}.nm{margin-top:40px}.of{font-size:18px}.og{margin-top:1.56em}.oh{line-height:28px}.oi{letter-spacing:-0.003em}.pd{margin-top:1.34em}.pj{font-size:16px}.pk{margin-top:1.23em}.px{margin-top:0.67em}.rc{margin-bottom:80px}.ro{display:inline-block}.ru{padding-top:48px}.sy{margin-top:32px}.mc:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{font-size:14px}.ed{line-height:20px}.ej{font-size:13px}.ek{padding:5px 12px}.ff{display:flex}.fw{margin-bottom:68px}.ga{max-width:680px}.hk{font-size:42px}.hl{margin-top:1.19em}.hm{margin-bottom:32px}.hn{line-height:52px}.ho{letter-spacing:-0.011em}.ic{align-items:center}.jk{border-top:solid 1px #F2F2F2}.jl{border-bottom:solid 1px #F2F2F2}.jm{margin:32px 0 0}.jn{padding:3px 8px}.jy> *{margin-right:24px}.jz> :last-child{margin-right:0}.lq{margin:0}.na{font-size:24px}.nb{margin-top:1.95em}.nc{line-height:30px}.nd{letter-spacing:-0.016em}.np{margin-top:56px}.or{font-size:20px}.os{margin-top:2.14em}.ot{line-height:32px}.ou{letter-spacing:-0.003em}.pg{margin-top:1.14em}.pq{margin-top:1.72em}.pr{line-height:24px}.ps{letter-spacing:0}.qa{margin-top:0.94em}.rf{margin-bottom:88px}.rx{padding-top:72px}.tb{margin-top:40px}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.ea{font-size:13px}.eb{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:68px}.fz{max-width:680px}.hf{font-size:42px}.hg{margin-top:1.19em}.hh{margin-bottom:32px}.hi{line-height:52px}.hj{letter-spacing:-0.011em}.ib{align-items:center}.jg{border-top:solid 1px #F2F2F2}.jh{border-bottom:solid 1px #F2F2F2}.ji{margin:32px 0 0}.jj{padding:3px 8px}.jw> *{margin-right:24px}.jx> :last-child{margin-right:0}.lp{margin:0}.mw{font-size:24px}.mx{margin-top:1.95em}.my{line-height:30px}.mz{letter-spacing:-0.016em}.no{margin-top:56px}.on{font-size:20px}.oo{margin-top:2.14em}.op{line-height:32px}.oq{letter-spacing:-0.003em}.pf{margin-top:1.14em}.pn{margin-top:1.72em}.po{line-height:24px}.pp{letter-spacing:0}.pz{margin-top:0.94em}.re{margin-bottom:88px}.rw{padding-top:72px}.ta{margin-top:40px}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dy{font-size:13px}.dz{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.ha{font-size:32px}.hb{margin-top:1.01em}.hc{margin-bottom:24px}.hd{line-height:38px}.he{letter-spacing:-0.014em}.ia{align-items:flex-start}.iy{flex-direction:column}.je{margin:24px 0 0}.jf{padding:0}.ju> *{margin-right:8px}.jv> :last-child{margin-right:8px}.km{margin-left:0px}.lo{margin:0}.me{border:1px solid #F2F2F2}.mf{border-radius:99em}.mg{padding:0px 16px 0px 12px}.mh{height:38px}.mi{align-items:center}.mk svg{margin-right:8px}.ms{font-size:20px}.mt{margin-top:1.2em}.mu{line-height:24px}.mv{letter-spacing:0}.nn{margin-top:40px}.oj{font-size:18px}.ok{margin-top:1.56em}.ol{line-height:28px}.om{letter-spacing:-0.003em}.pe{margin-top:1.34em}.pl{font-size:16px}.pm{margin-top:1.23em}.py{margin-top:0.67em}.rd{margin-bottom:80px}.rv{padding-top:48px}.sz{margin-top:32px}.mj:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="print">.rm{display:none}</style><style type="text/css" data-fela-rehydration="526" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ns{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><script type="text/javascript" async="" src="./Tutorial_files/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-RFB9oVlvTplM2seWHBz+PsmThT1DmbvbuROpMMcARnCHL3Ko4+efpawFtv6DvE/q"></script><script async="" src="https://cdn.branch.io/branch-latest.min.js"></script><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fda:true\u002Fresize:fit:1200\u002F0*9PSSJMGe60zzIkKE"],"url":"https:\u002F\u002Fmedium.com\u002F@bloomfountaincoder\u002Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9","dateCreated":"2024-03-10T06:23:50.330Z","datePublished":"2024-03-10T06:23:50.330Z","dateModified":"2024-03-13T05:13:20.085Z","headline":"Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.","name":"Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.","description":"Large language models (LLMs) typically have huge parameters running from hundreds of millions to billions due to the huge datasets they were trained on and several layers of their transformer model…","identifier":"388dcfb1c7e9","author":{"@type":"Person","name":"Hidiat Ibrahim","url":"https:\u002F\u002Fmedium.com\u002F@bloomfountaincoder"},"creator":["Hidiat Ibrahim"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fmedium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:539\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002F@bloomfountaincoder\u002Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9"}</script><script async="true" src="https://www.googletagmanager.com/gtag/js?id=G-7JY7T788PK" data-rh="true"></script><script data-rh="true">window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-7JY7T788PK');</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><script src="./Tutorial_files/enterprise.js" data-rh="true"></script><style data-fela-type="RULE" type="text/css" media="(orientation: landscape) and (max-width: 903.98px)"></style><style id="googleidentityservice_button_styles">.qJTHM{-webkit-user-select:none;color:#202124;direction:ltr;-webkit-touch-callout:none;font-family:"Roboto-Regular",arial,sans-serif;-webkit-font-smoothing:antialiased;font-weight:400;margin:0;overflow:hidden;-webkit-text-size-adjust:100%}.ynRLnc{left:-9999px;position:absolute;top:-9999px}.L6cTce{display:none}.bltWBb{word-break:break-all}.hSRGPd{color:#1a73e8;cursor:pointer;font-weight:500;text-decoration:none}.Bz112c-W3lGp{height:16px;width:16px}.Bz112c-E3DyYd{height:20px;width:20px}.Bz112c-r9oPif{height:24px;width:24px}.Bz112c-uaxL4e{-webkit-border-radius:10px;border-radius:10px}.LgbsSe-Bz112c{display:block}.S9gUrf-YoZ4jf,.S9gUrf-YoZ4jf *{border:none;margin:0;padding:0}.fFW7wc-ibnC6b>.aZ2wEe>div{border-color:#4285f4}.P1ekSe-ZMv3u>div:nth-child(1){background-color:#1a73e8!important}.P1ekSe-ZMv3u>div:nth-child(2),.P1ekSe-ZMv3u>div:nth-child(3){background-image:linear-gradient(to right,rgba(255,255,255,.7),rgba(255,255,255,.7)),linear-gradient(to right,#1a73e8,#1a73e8)!important}.haAclf{display:inline-block}.nsm7Bb-HzV7m-LgbsSe{-webkit-border-radius:4px;border-radius:4px;-webkit-box-sizing:border-box;box-sizing:border-box;-webkit-transition:background-color .218s,border-color .218s;transition:background-color .218s,border-color .218s;-webkit-user-select:none;-webkit-appearance:none;background-color:#fff;background-image:none;border:1px solid #dadce0;color:#3c4043;cursor:pointer;font-family:"Google Sans",arial,sans-serif;font-size:14px;height:40px;letter-spacing:0.25px;outline:none;overflow:hidden;padding:0 12px;position:relative;text-align:center;vertical-align:middle;white-space:nowrap;width:auto}@media screen and (-ms-high-contrast:active){.nsm7Bb-HzV7m-LgbsSe{border:2px solid windowText;color:windowText}}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe{font-size:14px;height:32px;letter-spacing:0.25px;padding:0 10px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe{font-size:11px;height:20px;letter-spacing:0.3px;padding:0 8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe{padding:0;width:40px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe{width:32px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe{width:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK{-webkit-border-radius:20px;border-radius:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.pSzOP-SxQuSe{-webkit-border-radius:16px;border-radius:16px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.purZT-SxQuSe{-webkit-border-radius:10px;border-radius:10px}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc{border:none;color:#fff}.nsm7Bb-HzV7m-LgbsSe.MFS4be-v3pZbf-Ia7Qfc{background-color:#1a73e8}.nsm7Bb-HzV7m-LgbsSe.MFS4be-JaPV2b-Ia7Qfc{background-color:#202124;color:#e8eaed}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:18px;margin-right:8px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:14px;min-width:14px;width:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:10px;min-width:10px;width:10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin-left:8px;margin-right:-4px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:10px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:4px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:3px;border-top-left-radius:3px;-webkit-border-bottom-left-radius:3px;border-bottom-left-radius:3px;display:-webkit-box;display:-webkit-flex;display:flex;justify-content:center;-webkit-align-items:center;align-items:center;background-color:#fff;height:36px;margin-left:-10px;margin-right:12px;min-width:36px;width:36px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c,.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:28px;margin-left:-8px;margin-right:10px;min-width:28px;width:28px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:16px;margin-left:-6px;margin-right:8px;min-width:16px;width:16px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:3px;border-radius:3px;margin-left:2px;margin-right:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:14px;border-radius:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:8px;border-radius:8px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-bN97Pc-sM5MNb{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;-webkit-flex-direction:row;flex-direction:row;justify-content:space-between;-webkit-flex-wrap:nowrap;flex-wrap:nowrap;height:100%;position:relative;width:100%}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX{justify-content:center}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:1;flex-grow:1;font-family:"Google Sans",arial,sans-serif;font-weight:500;overflow:hidden;text-overflow:ellipsis;vertical-align:top}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-weight:300}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:0;flex-grow:0}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-MJoBVe{-webkit-transition:background-color .218s;transition:background-color .218s;bottom:0;left:0;position:absolute;right:0;top:0}.nsm7Bb-HzV7m-LgbsSe:hover,.nsm7Bb-HzV7m-LgbsSe:focus{-webkit-box-shadow:none;box-shadow:none;border-color:#d2e3fc;outline:none}.nsm7Bb-HzV7m-LgbsSe:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.04)}.nsm7Bb-HzV7m-LgbsSe:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.1)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.24)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.32)}.nsm7Bb-HzV7m-LgbsSe .n1UuX-DkfjY{-webkit-border-radius:50%;border-radius:50%;display:-webkit-box;display:-webkit-flex;display:flex;height:20px;margin-left:-4px;margin-right:8px;min-width:20px;width:20px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-family:"Roboto";font-size:12px;text-align:left}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .ssJRIf,.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .fmcmS{overflow:hidden;text-overflow:ellipsis}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;color:#5f6368;fill:#5f6368;font-size:11px;font-weight:400}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.MFS4be-Ia7Qfc .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{color:#e8eaed;fill:#e8eaed}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .Bz112c{height:18px;margin:-3px -3px -3px 2px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:0;border-top-left-radius:0;-webkit-border-bottom-left-radius:0;border-bottom-left-radius:0;-webkit-border-top-right-radius:3px;border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;border-bottom-right-radius:3px;margin-left:12px;margin-right:-10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.L5Fo6c-sM5MNb{border:0;display:block;left:0;position:relative;top:0}.L5Fo6c-bF1uUb{-webkit-border-radius:4px;border-radius:4px;bottom:0;cursor:pointer;left:0;position:absolute;right:0;top:0}.L5Fo6c-bF1uUb:focus{border:none;outline:none}sentinel{}</style><link id="googleidentityservice" type="text/css" media="all" rel="stylesheet" href="./Tutorial_files/style"></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="l c"><div class="l m n o c" style="transform: translateY(0px);"><div class="p q r s t u v w x i d y z"><a class="dt ag du be ak b am an ao ap aq ar as at s u w i d q dv z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F388dcfb1c7e9&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;source=---two_column_layout_nav----------------------------------" rel="noopener follow">Open in app<svg width="10" height="10" viewBox="0 0 10 10" fill="none" class="ds"><path d="M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z" fill="currentColor"></path></svg></a><div class="ab q"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><button class="be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" data-testid="headerSignUpButton">Sign up</button></span></p><div class="aw l"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" rel="noopener follow" href="https://medium.com/?source=---two_column_layout_nav----------------------------------"><svg viewBox="0 0 3940 610" class="au av"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""></div></div></div><div class="h k w ff fg"><div class="fh ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" rel="noopener follow" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---two_column_layout_nav-----------------------new_post_topnav-----------"><div class="be b bf z dt fi fj ab q fk fl"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg><div class="ds l">Write</div></div></a></span></div></div><div class="k j i d"><div class="fh ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" rel="noopener follow" href="https://medium.com/search?source=---two_column_layout_nav----------------------------------"><div class="be b bf z dt fi fj ab q fk fl"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Search"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div></a></div></div><div class="fh h k j"><div class="ab q"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><button class="be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" data-testid="headerSignUpButton">Sign up</button></span></p><div class="aw l"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="l" aria-hidden="false"><button class="ax fm am ab q ao fn fo fp" aria-label="user options menu" data-testid="headerUserIcon"><div class="l fi"><img alt="" class="l fc bx by bz cw" src="./Tutorial_files/1_dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy" role="presentation"><div class="fq bx l by bz fr n ax fs"></div></div></button></div></div></div><div class="l"><div class="rm" role="dialog" aria-modal="true" tabindex="-1"><div class="to tp bg dv tq tr ts ao tt gl tu" aria-hidden="true" role="presentation"></div><div class="tv tq tw tx ty to dv fc tz ua ub lg uc ud ue uf ug uh ui uj uk" aria-hidden="true"></div></div><div class="ft fu fv fw fx l"><div class="ab ca"><div class="ch bg fy fz ga gb"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="fr gh gi gj gk gl"></div><div class="gm gn go gp gq"><div class="ab ca"><div class="ch bg fy fz ga gb"><div><h1 id="03e3" class="pw-post-title gr gs gt be gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht bj" data-testid="storyTitle" data-selectable-paragraph="">Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.</h1><div class="hu hv hw hx hy"><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="hz ia ib ic id ab"><div><div class="ab ie"><a rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=post_page-----388dcfb1c7e9--------------------------------"><div><div class="bl" aria-hidden="false" aria-describedby="1" aria-labelledby="1"><div class="l if ig bx ih ii"><div class="l fi"><img alt="Hidiat Ibrahim" class="l fc bx dc dd cw" src="./Tutorial_files/0_SuNGmT-nSEiriMxS.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"><div class="ij bx l dc dd fr n ik fs"></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="il ab q"><div class="ab q im"><div class="ab q"><div><div class="bl" aria-hidden="false" aria-describedby="2" aria-labelledby="2"><p class="be b in io bj"><a class="af ag ah ai aj ak al am an ao ap aq ar ip" data-testid="authorName" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=post_page-----388dcfb1c7e9--------------------------------">Hidiat Ibrahim</a></p></div></div></div><span class="iq ir" aria-hidden="true"><span class="be b bf z dt">·</span></span><p class="be b in io dt"><span><a class="is it ah ai aj ak al am an ao ap aq ar ew iu iv" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa22196dac4ca&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=post_page-a22196dac4ca----388dcfb1c7e9---------------------post_header-----------">Follow</a></span></p></div></div></span></div></div><div class="l iw"><span class="be b bf z dt"><div class="ab cm ix iy iz"><span class="be b bf z dt"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z dt">·</span></span></div><span data-testid="storyPublishDate">Mar 10, 2024</span></div></span></div></span></div></div></div><div class="ab co jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ff fg q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerClapButton" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----388dcfb1c7e9---------------------clap_footer-----------"><div><div class="bl" aria-hidden="false" aria-describedby="3" aria-labelledby="3"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="60" aria-labelledby="60"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">11<span class="l h g f rr rs"></span></button></p></div></div></div></div></div><div><div class="bl" aria-hidden="false" aria-describedby="4" aria-labelledby="4"><button class="ao ko lg lh ab q fj li lj" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="lf"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">1</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"></div><div class="h k"><div><div class="bl" aria-hidden="false" aria-describedby="5" aria-labelledby="5"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerBookmarkButton" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;source=-----388dcfb1c7e9---------------------bookmark_footer-----------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div><div class="fc lm cm"><div class="l ae"><div class="ab ca"><div class="ln lo lp lq lr ls ch bg"><div class="ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;source=-----388dcfb1c7e9---------------------post_audio_button-----------"><div><div class="bl" aria-hidden="false" aria-describedby="63" aria-labelledby="63"><button aria-label="Listen" data-testid="audioPlayButton" class="af fj ah ai aj ak al lt an ao ap ew lu lv lj lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dt">Listen</p></div></button></div></div></a></span></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false" aria-describedby="7" aria-labelledby="7"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af fj ah ai aj ak al lt an ao ap ew lu lv lj lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dt">Share</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="428c" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Fine-tune Falcon 7B LLM.</h1><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt fi nu bg nv"><div class="nj nk nl"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9PSSJMGe60zzIkKE 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*9PSSJMGe60zzIkKE 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*9PSSJMGe60zzIkKE 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*9PSSJMGe60zzIkKE 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*9PSSJMGe60zzIkKE 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*9PSSJMGe60zzIkKE 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9PSSJMGe60zzIkKE 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/0*9PSSJMGe60zzIkKE 640w, https://miro.medium.com/v2/resize:fit:720/0*9PSSJMGe60zzIkKE 720w, https://miro.medium.com/v2/resize:fit:750/0*9PSSJMGe60zzIkKE 750w, https://miro.medium.com/v2/resize:fit:786/0*9PSSJMGe60zzIkKE 786w, https://miro.medium.com/v2/resize:fit:828/0*9PSSJMGe60zzIkKE 828w, https://miro.medium.com/v2/resize:fit:1100/0*9PSSJMGe60zzIkKE 1100w, https://miro.medium.com/v2/resize:fit:1400/0*9PSSJMGe60zzIkKE 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg ls nw c" width="700" height="504" loading="eager" role="presentation" src="./Tutorial_files/0_9PSSJMGe60zzIkKE.jpg"></picture></div></div><figcaption class="nx fe ny nj nk nz oa be b bf z dt" data-selectable-paragraph="">Photo by <a class="af ob" href="https://unsplash.com/@dawson2406?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stephen Dawson</a> on <a class="af ob" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1679" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">PEFT / LoRA / QLoRA</p><p id="5357" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Large language models (LLMs) typically have huge parameters running from hundreds of millions to billions due to the huge datasets they were trained on and several layers of their transformer model architecture. This makes fine-tuning all the layers of the model computationally prohibitive. There have been several efforts to find a way to fine-tune LLMs without the high computation costs. This gave rise to several methods such as parameter-efficient fine-tuning. Parameter-efficient fine-tuning (PEFT) comprises different techniques such as prompt-tuning, prefix-tuning, p-tuning, low-rank adaptation, and quantization with low-rank adaptation(QLoRA).</p><p id="5251" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">LoRA (Low-Rank Adaptation) is a reparameterization method that decomposes the weight change matrix of an LLM into low-rank matrices. These low-rank matrices are typically inserted in the attention blocks of the model. The original weight matrix of the pre-trained model is frozen and only the inserted smaller matrices are updated during training. This reduces the number of trainable parameters, reducing memory usage and training time which can be very expensive for large models.</p><p id="daec" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">In this notebook, we’ll fine-tune Falcon LLM 7b with QLoRA. QLoRA adds quantization to LoRA by loading the base model in 4-bit floating point precision before applying LoRA. This reduces the memory consumption of LLM fine-tuning without a significant reduction in performance. Quantization is a technique in which the floating point precision of the parameters is reduced from 32 bits to up to 4 bits, without losing a lot of information.</p><p id="6613" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Falcon-7B is a 7B parameters causal decoder-only model that was trained on 1,500B tokens of <a class="af ob" href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" rel="noopener ugc nofollow" target="_blank">RefinedWeb</a> enhanced with curated corpora. FalconLLM was developed by the Technology Innovation Institute, UAE. Falcon LLM is open-source and it’s available on the Hugging Face website.</p><p id="2e82" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We are going to run this notebook on the free tier T4 GPU on Google Colab. This compute resource is still insufficient for proper fine-tuning of LLM and because of the limited compute resources, we’ll use a <a class="af ob" href="https://huggingface.co/ybelkada/falcon-7b-sharded-bf16/tree/main" rel="noopener ugc nofollow" target="_blank">sharded version</a> of Falcon LLM available on Hugging Face contributed by <a class="af ob" href="https://huggingface.co/ybelkada" rel="noopener ugc nofollow" target="_blank">Younes Belkada</a>.</p><p id="c4c3" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll follow these steps:</p><ol class=""><li id="60d9" class="oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bj" data-selectable-paragraph="">Install and load libraries and dataset</li><li id="c30b" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Load the base model with quantization (for QLoRA)</li><li id="9885" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Configure LoRA adapter</li><li id="c858" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Set up training arguments</li><li id="7fdf" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Train the model</li><li id="47cb" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Save the model</li><li id="3469" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Create generation config for prediction</li><li id="1f28" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz pa pb pc bj" data-selectable-paragraph="">Make inferences on sample test data</li></ol><h2 id="f8dd" class="pi mm gt be mn pj pk dx mr pl pm dz mv on pn po pp or pq pr ps ov pt pu pv pw bj" data-selectable-paragraph="">Libraries</h2><p id="eec3" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">We need to import some libraries to implement QLoRA.</p><p id="a84b" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">1. Transformer Reinforcement Learning (trl). This helps us train the language model with reinforcement learning. It’s integrated with huggingface transformers. TRL supports decoder models such as GPT-2, BLOOM, and Falcon LLM.</p><p id="f4d4" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">2. PEFT. This library is used for efficiently adapting pre-trained language models by fine-tuning only a small number of model parameters significantly reducing computational and storage costs.</p><p id="2c35" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">3. Sharded Falcon LLM. Available on huggingface. The sharded falcon model loads faster on low compute than the original model.</p><p id="6814" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/ybelkada/falcon-7b-sharded-bf16/tree/main" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/ybelkada/falcon-7b-sharded-bf16/tree/main</a></p><p id="89a1" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">4. Accelerate makes the Pytorch training loop faster.</p><p id="7d0e" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">5. Bitsandbytes is a lightweight wrapper around CUDA custom functions, particularly 8-bit optimizers and quantization functions. It’s used to handle the quantization process in QLoRA. It’s developed by HuggingFace.</p><p id="6385" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">6. Einops simplifies tensor operations.</p><p id="bc9c" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">7. Datasets makes it easy to load datasets from Huggingface datasets repository.</p><p id="2663" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">8. Transformers is the standard Huggingface library for accessing pre-trained models on Huggingface using python.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="982e" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">!pip install -q trl transformers accelerate peft datasets bitsandbytes einops</span></pre><h1 id="a3d5" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Import the libraries</h1><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="b185" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> transformers<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTTrainer<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, PeftModel, PeftConfig<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Dataset, DatasetDict<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span>  prepare_model_for_kbit_training, get_peft_model, TaskType<br><br><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br>    AutoModelForCausalLM,<br>    AutoTokenizer,<br>    BitsAndBytesConfig,<br>    AutoTokenizer,<br>    TrainingArguments,<br>    pipeline,<br>    logging<br>    )</span></pre><h1 id="764d" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Dataset</h1><p id="d326" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">E-commerce customer sentiment analysis data. <a class="af ob" href="https://huggingface.co/datasets/arize-ai/ecommerce_reviews_with_language_drift?row=13" rel="noopener ugc nofollow" target="_blank">Data</a></p><p id="0dfd" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Load the dataset and convert it into a dataframe so that we can work on it with pandas and scikit-learn.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="4d24" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data = load_dataset(<span class="hljs-string">'arize-ai/ecommerce_reviews_with_language_drift'</span>, split=<span class="hljs-string">'validation'</span>)</span></pre><pre class="ql qc qd qe bo qf ba bj"><span id="3903" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data = pd.DataFrame(data)<br>data.head(<span class="hljs-number">10</span>)</span></pre><p id="015d" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Extracting the two columns we need out of the dataframe.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="95c2" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data = data[[<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>]]<br>data.head()</span></pre><p id="7dcb" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Convert the label from number to text.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="bb57" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data[<span class="hljs-string">'label'</span>] = data[<span class="hljs-string">'label'</span>].replace([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-string">'negative'</span>, <span class="hljs-string">'neutral'</span>, <span class="hljs-string">'positive'</span>])</span></pre><p id="1ce8" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll format the data to include both the text input and the expected output and then, train LLM with the new combination. The LLM will learn from this combined data.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="7987" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data[<span class="hljs-string">'formatted_data'</span>] = data.apply(<span class="hljs-keyword">lambda</span> row: <span class="hljs-built_in">str</span>(row[<span class="hljs-string">'text'</span>]) + <span class="hljs-string">" -&gt;: "</span> + row[<span class="hljs-string">'label'</span>], axis = <span class="hljs-number">1</span>)<br>data.head()</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt fi nu bg nv"><div class="nj nk qm"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k6t0LnKfJ77sUIHBGcjdSQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*k6t0LnKfJ77sUIHBGcjdSQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*k6t0LnKfJ77sUIHBGcjdSQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*k6t0LnKfJ77sUIHBGcjdSQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*k6t0LnKfJ77sUIHBGcjdSQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*k6t0LnKfJ77sUIHBGcjdSQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*k6t0LnKfJ77sUIHBGcjdSQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*k6t0LnKfJ77sUIHBGcjdSQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg ls nw c" width="700" height="213" loading="lazy" role="presentation" src="./Tutorial_files/1_k6t0LnKfJ77sUIHBGcjdSQ.png"></picture></div></div></figure><p id="046b" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Let’s view sample data from the new column.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="8ea0" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">data[<span class="hljs-string">'formatted_data'</span>][<span class="hljs-number">0</span>]</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt fi nu bg nv"><div class="nj nk qn"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrYUF3cRKxRKSEYTk45M7A.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*TrYUF3cRKxRKSEYTk45M7A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*TrYUF3cRKxRKSEYTk45M7A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*TrYUF3cRKxRKSEYTk45M7A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*TrYUF3cRKxRKSEYTk45M7A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*TrYUF3cRKxRKSEYTk45M7A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*TrYUF3cRKxRKSEYTk45M7A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*TrYUF3cRKxRKSEYTk45M7A.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg ls nw c" width="700" height="22" loading="lazy" role="presentation" src="./Tutorial_files/1_TrYUF3cRKxRKSEYTk45M7A.png"></picture></div></div></figure><p id="1a17" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We are using only a few data points because we have low computation resources. Let’s split the data into train and test splits.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="f23e" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">train_df, test_df = train_test_split(data, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)</span></pre><p id="67e2" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Hugging Face transformer models expect the training data in DatasetDict format.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="6732" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">train_dict = DatasetDict({<br>    <span class="hljs-string">'train'</span>: Dataset.from_pandas(train_df)<br>})</span></pre><h1 id="aada" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Load the base model with quantization.</h1><p id="a318" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">In the code snippet below, we’ll use the bitsandbytes library to quantize the model. Bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication, and quantization functions.</p><p id="b3ed" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll set the `<em class="qo">bitsandbytes`</em> configuration to load the model in 4-bit.</p><p id="bfa2" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Next, we’ll load the model with the hugging face class `<em class="qo">AutoModelForCausalLM`</em> (for the next text generation) and pass the quantization configuration into the model. We’ll set `<em class="qo">trust_remote_code`</em> to True because we are accessing it via huggingface.</p><p id="a444" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll set `<em class="qo">model.config.use_cache`</em> to false because the KV cache is not useful during training (Finetune) since the weights will be updated. The cache is set to true during inference only.</p><p id="2355" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">The `<em class="qo">prepare_model_for_kbit_training`</em> function makes the model available for training.</p><p id="bbc5" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`Double quantization`</em> is the process of quantizing the quantization constants used during the quantization process in the 4-bit NF quantization. This can save 0.5 bits per parameter on average, as mentioned in the paper.</p><p id="2393" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll enable gradient checkpointing to reduce the number of stored activations and thus, save memory. However, this leads to a slower backward pass.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="fde6" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">model_name = <span class="hljs-string">"ybelkada/falcon-7b-sharded-bf16"</span><br><br>bnb_config = BitsAndBytesConfig(<br>    load_in_4bit=<span class="hljs-literal">True</span>,<br>    bnb_4bit_quant_type=<span class="hljs-string">"nf4"</span>,<br>    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,<br>    bnb_4bit_compute_dtype=torch.float16,<br>)<br><br>model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    quantization_config=bnb_config,<br>    trust_remote_code=<span class="hljs-literal">True</span>,<br>    device_map={<span class="hljs-string">""</span>:<span class="hljs-number">0</span>}<br>)<br><br>model.gradient_checkpointing_enable()<br><span class="hljs-comment"># Prepares the model for kbit training</span><br>model = prepare_model_for_kbit_training(model)</span></pre><p id="bb74" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Print the number of trainable and total parameters in the model.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="1d41" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-keyword">def</span> <span class="hljs-title.function">print_trainable_parameters</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-string">"""<br>    Prints the number of trainable parameters in the model.<br>    """</span><br>    trainable_params = <span class="hljs-number">0</span><br>    all_param = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        all_param += param.numel()<br>        <span class="hljs-keyword">if</span> param.requires_grad:<br>            trainable_params += param.numel()<br>    <span class="hljs-built_in">print</span>(<br>        <span class="hljs-string">f"trainable params: <span class="hljs-subst">{trainable_params}</span> || all params: <span class="hljs-subst">{all_param}</span> || trainable%: <span class="hljs-subst">{<span class="hljs-number">100</span> * trainable_params / all_param}</span>"</span><br>        )</span></pre><h1 id="b13c" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Configure LoRA Adapter.</h1><p id="b1c0" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">LoRA uses the adapter technique to add new smaller trainable parameters to the model. In the code snippet below, we’ll configure the LoRA adapter. The LoRA adapter works by reparameterizing the weights of a layer matrix usually the linear layers. For the best performance, we’ll include all linear layers in the target modules.</p><p id="eedb" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`lora_alpha`</em>: This is the scaling factor for the LoRA update matrices. The higher the value of lora_alpha the more aggressive the updates to the weights.</p><p id="b5ee" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`lora_dropout`</em>: This is the dropout percentage for the LoRA layers.</p><p id="2bb4" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">`<em class="qo">r`</em>: This is the rank of the update matrices. Lower rank results in smaller update matrices with fewer trainable parameters. A higher rank will result in larger matrices, which can hold more information about the weights of the layer matrix.</p><p id="90fe" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">`<em class="qo">bias`</em>: This determines whether the bias parameters should be updated during training. The bias parameters are the weights that are added to the output of a layer. The value can be ‘none’, ‘all’, or ‘lora_only’. We’ll choose none to preserve the base model output.</p><p id="d98e" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`task_type`</em>: This specifies the task type for which the LoRA adapter is being used. We’ll choose “CAUSAL_LM”. Other values for `<em class="qo">task_type`</em> could be “NLI” or “MT”</p><p id="639c" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">`<em class="qo">target_modules`</em>: These are the modules to which the LoRA update matrices will be applied. The target modules are the layers in the base model that will be reparameterized by the LoRA adapter. Other values for target_modules could be [“attention”, “dense_final”] or [“query_key_value”, “dense”, “dense_h_to_4h”].</p><p id="427f" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Next, we’ll create an object of the LoraConfig class and pass in the selected parameters.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="efa3" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">lora_alpha = <span class="hljs-number">32</span><br>lora_dropout = <span class="hljs-number">0.05</span><br>lora_r = <span class="hljs-number">8</span><br><br>lora_config = LoraConfig(<br>    lora_alpha=lora_alpha,<br>    lora_dropout=lora_dropout,<br>    r=lora_r,<br>    bias=<span class="hljs-string">"none"</span>,<br>    task_type=<span class="hljs-string">"CAUSAL_LM"</span>,<br>    target_modules=[<br>        <span class="hljs-string">"query_key_value"</span>,<br>        <span class="hljs-string">"dense"</span>,<br>        <span class="hljs-string">"dense_h_to_4h"</span>,<br>        <span class="hljs-string">"dense_4h_to_h"</span>,<br>    ])</span></pre><p id="88d8" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Combine the quantized base model with the LoRA adapter using `<em class="qo">get_peft_model`</em> and pass the lora_config along with the pre-trained Falcon base model.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="50e1" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-comment"># Now you get a model ready for QLoRA training</span><br>lora_model = get_peft_model(model, lora_config)<br>lora_model.print_trainable_parameters()</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt fi nu bg nv"><div class="nj nk qp"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDk7Yoj1KZs4x-iVsss6Nw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*fDk7Yoj1KZs4x-iVsss6Nw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fDk7Yoj1KZs4x-iVsss6Nw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fDk7Yoj1KZs4x-iVsss6Nw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fDk7Yoj1KZs4x-iVsss6Nw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fDk7Yoj1KZs4x-iVsss6Nw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fDk7Yoj1KZs4x-iVsss6Nw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*fDk7Yoj1KZs4x-iVsss6Nw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg ls nw c" width="700" height="33" loading="lazy" role="presentation" src="./Tutorial_files/1_fDk7Yoj1KZs4x-iVsss6Nw.png"></picture></div></div><figcaption class="nx fe ny nj nk nz oa be b bf z dt" data-selectable-paragraph="">Number of trainable and total parameters</figcaption></figure><p id="fd95" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We’ll fine-tune the constructed custom LoRA model using huggingface Trainer API. First, we’ll set the training arguments.</p><h1 id="a6b4" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Set up training Arguments.</h1><p id="64b4" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`output_dir`</em> defines the directory where the training results will be written.</p><p id="7239" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`per_device_train_batch_size`</em> defines the batch size for each GPU. This could be increased or decreased, depending on the available GPU memory.</p><p id="6bd2" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`gradient_accumulation_steps`</em> defines the number of update steps to accumulate the gradients before performing a backward/update pass. This is used to increase the effective batch size without increasing the GPU memory usage.</p><p id="330c" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`optim`</em> defines the optimizer that will be used for training. The `<em class="qo">paged_adamw_32bit`</em> optimizer is a variant of `<em class="qo">AdamW`</em> that is designed to be more efficient on 32-bit GPUs.</p><p id="3904" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`save_steps`</em> defines the number of steps after which the model checkpoint will be saved.</p><p id="3fde" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`fp16`</em> defines whether to use 16-bit floating point precision during training. This can significantly reduce memory usage, but it may also reduce the accuracy of the model.</p><p id="d526" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`logging_steps`</em> defines the Number of update steps between two logs.</p><p id="5bc7" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`learning_rate`</em> defines the initial learning rate for the optimizer.</p><p id="0297" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`max_grad_norm`</em> defines the maximum norm of the gradients. This is used to prevent the gradients from becoming too large to prevent instability.</p><p id="8f47" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`max_steps`</em> defines the maximum number of steps to train for.</p><p id="6e96" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`warmup_ratio`</em> defines the ratio of the warm-up steps to the total number of steps. Warm-up steps gradually increase the learning rate, which helps to prevent the model from overfitting.</p><p id="eaba" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`lr_scheduler_type`</em> defines the type of learning rate scheduler that will be used. The `<em class="qo">constant scheduler`</em> keeps the learning rate constant for the entire training process. This could be increased or decreased, depending on the available GPU memory. Other values are `<em class="qo">cosine_scheduler`</em>, `<em class="qo">linear_scheduler`</em>, etc.</p><p id="157f" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">You may need to experiment with different values to determine the best values of the parameters for the specific language model you choose, the downstream task, and the available resources.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="11ca" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">training_arguments = TrainingArguments(<br>    output_dir=outputs,<br>    per_device_train_batch_size=<span class="hljs-number">4</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">4</span>,<br>    optim=<span class="hljs-string">"paged_adamw_32bit"</span>,<br>    save_steps=<span class="hljs-number">10</span>,<br>    logging_steps=<span class="hljs-number">10</span>,<br>    learning_rate=<span class="hljs-number">2e-4</span>,<br>    fp16=<span class="hljs-literal">True</span>,<br>    max_grad_norm=<span class="hljs-number">0.3</span>,<br>    max_steps=<span class="hljs-number">150</span>,<br>    warmup_ratio=<span class="hljs-number">0.03</span>,<br>    group_by_length=<span class="hljs-literal">True</span>,<br>    lr_scheduler_type=constant<br>)</span></pre><p id="b2a7" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Get the tokenizer specific to the pre-trained model and set the padding token to be the same as the end-of-sequence token.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="0906" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=<span class="hljs-literal">True</span>)<br>tokenizer.pad_token = tokenizer.eos_token</span></pre><p id="a1ae" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">In the following code snippet, we’ll initialize an object of the huggingface <em class="qo">`SFTTrainer`</em> class, and pass both the dataset and the data column that we want to train the model on, for our specific use case. We’ll also pass the peft configurations as inputs (to use the Lora configuration that we set earlier), tokenizer, maximum sequence length, model, and training arguments.</p><p id="82f5" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">`SFTTrainer`</em> is specifically optimized for Supervised Fine-tuning (SFT). It inherits from the Trainer class available in the Transformer library. We’ll import it from the `t<em class="qo">rl`</em> library.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="1e16" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">max_seq_length = <span class="hljs-number">512</span><br><br>trainer = SFTTrainer(<br>    model=lora_model,<br>    train_dataset=train_dict[<span class="hljs-string">'train'</span>],<br>    peft_config=lora_config,<br>    dataset_text_field=<span class="hljs-string">"formatted_data"</span>,<br>    max_seq_length=max_seq_length,<br>    tokenizer=tokenizer,<br>    args=training_arguments,<br>)</span></pre><p id="3c88" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">We will freeze the original weights of the model and keep the layer norm in float 32. Models are typically trained on 32-bit precision for higher accuracy. This step ensures more stable training. Following this, we will proceed with training the model.</p><p id="11da" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">This will cast the weights to higher precision floats in the layers at the time of computation resulting in a higher speed of fine-tuning.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="960e" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-comment"># Loop through the named modules of the model</span><br><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> trainer.model.named_modules():<br><span class="hljs-comment"># Check if the name contains "norm"</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">"norm"</span> <span class="hljs-keyword">in</span> name:<br> <span class="hljs-comment"># Convert the module to use torch.float32 data type</span><br>        module = module.to(torch.float32)</span></pre><h1 id="79a5" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Train the model.</h1><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="99ff" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-comment"># Disabling cache usage in the model configuration</span><br>lora_model.config.use_cache = <span class="hljs-literal">False</span><br><br>trainer.train()</span></pre><h1 id="9bc8" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Save the model.</h1><p id="e0bc" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">This will only save the LoRA model adapter. For inference, we need to load both the saved adapter and the base Falcon 7B model. The `<em class="qo">tuned_model</em>` directory contains the adapter bin and config files that are generated at the end of the training.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="4031" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">lora_model.save_pretrained(<span class="hljs-string">"tuned_model/"</span>)</span></pre><h1 id="e805" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">Inference</h1><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="3f4f" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">peft_model = <span class="hljs-string">'./tuned_model'</span><br>config = PeftConfig.from_pretrained(peft_model)<br><br>peft_base_model = AutoModelForCausalLM.from_pretrained(<br>    config.base_model_name_or_path,<br>    return_dict=<span class="hljs-literal">True</span>,<br>    quantization_config=bnb_config,<br>    trust_remote_code=<span class="hljs-literal">True</span>,<br>    device_map=<span class="hljs-string">'auto'</span><br>    )<br><br><span class="hljs-comment"># Load the Lora model</span><br>trained_model = PeftModel.from_pretrained(peft_base_model, peft_model)<br>trained_model_tokenizer = AutoTokenizer.from_pretrained(<br>    config.base_model_name_or_path,<br>    trust_remote_code=<span class="hljs-literal">True</span><br>)<br><br>trained_model_tokenizer.pad_token = trained_model_tokenizer.eos_toke</span></pre><p id="3130" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Select a sample text from the test dataset for inference.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="a92a" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">sample = test_df.iloc[<span class="hljs-number">0</span>, :]<br>sample_text = sample[<span class="hljs-string">'text'</span>]</span></pre><p id="273b" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Tokenize the sample text.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="1fde" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">batch = tokenizer(sample_text, return_tensors=<span class="hljs-string">'pt'</span>).to(<span class="hljs-string">"cuda"</span>)</span></pre><h2 id="f774" class="pi mm gt be mn pj pk dx mr pl pm dz mv on pn po pp or pq pr ps ov pt pu pv pw bj" data-selectable-paragraph="">Create generation config for prediction.</h2><p id="3928" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph="">We need to create a generation configuration for the inference.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="eed5" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">gen_config = GenerationConfig(<br>    max_new_tokens = <span class="hljs-number">5</span>,<br>    attention_mask=batch.attention_mask,<br>    pad_token_id = trained_model_tokenizer.pad_token_id,<br>    eos_token_id = trained_model_tokenizer.eos_token_id,<br>    repetition_penalty=<span class="hljs-number">2.0</span>,<br>    num_return_sequences=<span class="hljs-number">1</span><br> )</span></pre><p id="169f" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Pass the generation configuration into pytorch’s `<em class="qo">inference_mode</em>`. Pytorch inference mode is a better version of the torch.no_grad which disables computing gradients.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="b2f8" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph=""><span class="hljs-keyword">with</span> torch.inference_mode():<br>    result = trained_model.generate(<br>        input_ids=batch.input_ids,<br>        generation_config=gen_config,<br>    )<br><br>final_output = trained_model_tokenizer.decode(result[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)</span></pre><p id="14ad" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">Here’s is the generated output.</p><pre class="nm nn no np nq qc qd qe bo qf ba bj"><span id="f5c2" class="qg mm gt qd b bf qh qi l qj qk" data-selectable-paragraph="">final_output</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt fi nu bg nv"><div class="nj nk qq"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XN1EXhUFkfFBo_OOuCeFlA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*XN1EXhUFkfFBo_OOuCeFlA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XN1EXhUFkfFBo_OOuCeFlA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XN1EXhUFkfFBo_OOuCeFlA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XN1EXhUFkfFBo_OOuCeFlA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XN1EXhUFkfFBo_OOuCeFlA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XN1EXhUFkfFBo_OOuCeFlA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*XN1EXhUFkfFBo_OOuCeFlA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg ls nw c" width="700" height="20" loading="lazy" role="presentation" src="./Tutorial_files/1_XN1EXhUFkfFBo_OOuCeFlA.png"></picture></div></div><figcaption class="nx fe ny nj nk nz oa be b bf z dt" data-selectable-paragraph="">Final output.</figcaption></figure><p id="19a6" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">I hope this article explained the QLoRA fine-tuning simply. You may want to learn more from the references below so that you can experiment further with configurations of the libraries used in this article.</p><h1 id="2d2c" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">References.</h1><p id="2f1d" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one</a></p><p id="fa67" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fgeneration_strategies" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/generation_strategies</a></p><p id="c1dc" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></p><p id="85e4" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/docs/peft/en/package_reference/lora" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/peft/en/package_reference/lora</a></p><p id="bf1f" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/tiiuae/falcon-7b" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/tiiuae/falcon-7b</a></p><p id="b350" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph=""><a class="af ob" href="https://huggingface.co/blog/peft" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/peft</a></p><p id="0096" class="pw-post-body-paragraph oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz gm bj" data-selectable-paragraph="">I look forward to your comments on this article.</p><h1 id="d0ac" class="ml mm gt be mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bj" data-selectable-paragraph="">In Plain English 🚀</h1><p id="eba7" class="pw-post-body-paragraph oc od gt oe b of px oh oi oj py ol om on pz op oq or qa ot ou ov qb ox oy oz gm bj" data-selectable-paragraph=""><em class="qo">Thank you for being a part of the </em><a class="af ob" href="https://plainenglish.io/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu"><em class="qo">In Plain English</em></strong></a><em class="qo"> community! Before you go:</em></p><ul class=""><li id="18c0" class="oc od gt oe b of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz qr pb pc bj" data-selectable-paragraph="">Be sure to <strong class="oe gu">clap</strong> and <strong class="oe gu">follow</strong> the writer ️👏<strong class="oe gu">️️</strong></li><li id="5707" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz qr pb pc bj" data-selectable-paragraph="">Follow us: <a class="af ob" href="https://twitter.com/inPlainEngHQ" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">X</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://www.linkedin.com/company/inplainenglish/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">LinkedIn</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">YouTube</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://discord.gg/in-plain-english-709094664682340443" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">Discord</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://newsletter.plainenglish.io/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">Newsletter</strong></a></li><li id="8d35" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz qr pb pc bj" data-selectable-paragraph="">Visit our other platforms: <a class="af ob" href="https://stackademic.com/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">Stackademic</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://cofeed.app/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">CoFeed</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://venturemagazine.net/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">Venture</strong></a><strong class="oe gu"> | </strong><a class="af ob" href="https://blog.cubed.run/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">Cubed</strong></a></li><li id="9456" class="oc od gt oe b of pd oh oi oj pe ol om on pf op oq or pg ot ou ov ph ox oy oz qr pb pc bj" data-selectable-paragraph="">More content at <a class="af ob" href="https://plainenglish.io/" rel="noopener ugc nofollow" target="_blank"><strong class="oe gu">PlainEnglish.io</strong></a></li></ul></div></div></div></div></section></div></div></article><div class="ab ca"><div class="ch bg fy fz ga gb"></div></div></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="qs qt ab iz"><div class="qu ab"><a class="qv ax am ao" rel="noopener follow" href="https://medium.com/tag/llm?source=post_page-----388dcfb1c7e9---------------llm-----------------"><div class="qw fi cw qx gd qy qz be b bf z bj ra">Llm</div></a></div><div class="qu ab"><a class="qv ax am ao" rel="noopener follow" href="https://medium.com/tag/fine-tuning-llm?source=post_page-----388dcfb1c7e9---------------fine_tuning_llm-----------------"><div class="qw fi cw qx gd qy qz be b bf z bj ra">Fine Tuning Llm</div></a></div><div class="qu ab"><a class="qv ax am ao" rel="noopener follow" href="https://medium.com/tag/falcon-llm?source=post_page-----388dcfb1c7e9---------------falcon_llm-----------------"><div class="qw fi cw qx gd qy qz be b bf z bj ra">Falcon Llm</div></a></div><div class="qu ab"><a class="qv ax am ao" rel="noopener follow" href="https://medium.com/tag/machine-learning?source=post_page-----388dcfb1c7e9---------------machine_learning-----------------"><div class="qw fi cw qx gd qy qz be b bf z bj ra">Machine Learning</div></a></div><div class="qu ab"><a class="qv ax am ao" rel="noopener follow" href="https://medium.com/tag/transformers?source=post_page-----388dcfb1c7e9---------------transformers-----------------"><div class="qw fi cw qx gd qy qz be b bf z bj ra">Transformers</div></a></div></div></div></div><div class="l"></div><footer class="rb rc rd re rf rg rh ri rj ab q rk rl c"><div class="l ae"><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="ab co rm"><div class="ab q ki"><div class="rn l"><span class="l ro rp rq e d"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----388dcfb1c7e9---------------------clap_footer-----------"><div><div class="bl" aria-hidden="false" aria-describedby="8" aria-labelledby="8"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="66" aria-labelledby="66"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">11<span class="l h g f rr rs"></span></button></p></div></div></div></div></span><span class="l h g f rr rs"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----388dcfb1c7e9---------------------clap_footer-----------"><div><div class="bl" aria-hidden="false" aria-describedby="9" aria-labelledby="9"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="68" aria-labelledby="68"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">11</button></p></div></div></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false" aria-describedby="10" aria-labelledby="10"><button class="ao ko lg lh ab q fj li lj" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="lf"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b bf z dt"><span class="pw-responses-count le lf">1</span></p></button></div></div></div></div><div class="ab q"><div class="rt l iw"><div><div class="bl" aria-hidden="false" aria-describedby="11" aria-labelledby="11"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerBookmarkButton" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F388dcfb1c7e9&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;source=--------------------------bookmark_footer-----------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div><div class="rt l iw"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false" aria-describedby="12" aria-labelledby="12"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af fj ah ai aj ak al lt an ao ap ew lu lv lj lw"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class="ru rv rw rx ry l bw"><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="ck ab rz co"><div class="ab ie"><a rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=post_page-----388dcfb1c7e9--------------------------------"><div class="l sa sb bx sc ii"><div class="l fi"><img alt="Hidiat Ibrahim" class="l fc bx sd se cw" src="./Tutorial_files/0_SuNGmT-nSEiriMxS(1).png" width="72" height="72" loading="lazy"><div class="ij bx l sd se fr n ik fs"></div></div></div></a></div><div class="j i d"><div class="ab"><span><button class="be b bf z eo qw ep eq er es et eu ev ew ex ey ez sf fa fb fc bl fd fe">Follow</button></span><div class="ds l"><div><div><div class="bl" aria-hidden="false" aria-describedby="107" aria-labelledby="107"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa22196dac4ca%2Flazily-enable-writer-subscription&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----388dcfb1c7e9---------------------subscribe_user-----------"><button class="be b bf z eo am ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="un sh si"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=post_page-----388dcfb1c7e9--------------------------------"><h2 class="pw-author-name be ss st su sv bj"><span class="gm">Written by <!-- -->Hidiat Ibrahim</span></h2></a></div><div class="qu ab"><div class="l iw"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar ip" rel="noopener follow" href="https://medium.com/@bloomfountaincoder/followers?source=post_page-----388dcfb1c7e9--------------------------------">1 Follower</a></span></div></div><div class="ql l"><p class="be b bf z bj">I develop machine learning, NLP, LLM, and computer vision apps. Certified AWS ML. Freelancer at Upwork. <a class="af ag ah ai aj ak al am an ao ap aq ar ob gn" href="https://www.upwork.com/freelancers/~0124f2a56984ab4a4b" rel="noopener  ugc nofollow">https://www.upwork.com/freelancers/~0124f2a56984ab4a4b</a></p></div></div><div class="h k"><div class="ab"><span><button class="be b bf z eo qw ep eq er es et eu ev ew ex ey ez sf fa fb fc bl fd fe">Follow</button></span><div class="ds l"><div><div><div class="bl" aria-hidden="false" aria-describedby="108" aria-labelledby="108"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa22196dac4ca%2Flazily-enable-writer-subscription&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----388dcfb1c7e9---------------------subscribe_user-----------"><button class="be b bf z eo am ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="un sh si"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="sw bg sx sy sz ta tb tc"></div></div></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="uo up l"><h2 class="be ss in z gs bj">More from Hidiat Ibrahim</h2></div><div class="uq ab ki iz ur us ut uu uv uw ux uy uz va vb vc vd ve vf"><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@bloomfountaincoder/question-answering-on-pdf-with-google-palm-2-and-langchain-094ac604d68a" tabindex="0"><div class="wt"><div aria-label="Question-Answering on pdf with Google PaLM 2 and Langchain."><div class="wv ww wx wy wz"><img alt="Question-Answering on pdf with Google PaLM 2 and Langchain." class="bg xa xb xc xd bw" src="./Tutorial_files/0_6TIxryCYymYG6XC6.jpg" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="109" aria-labelledby="109"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=author_recirc-----388dcfb1c7e9----0---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div class="l fi"><img alt="Hidiat Ibrahim" class="l fc bx xo xp cw" src="./Tutorial_files/0_SuNGmT-nSEiriMxS(2).png" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="110" aria-labelledby="110"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=author_recirc-----388dcfb1c7e9----0---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Hidiat Ibrahim</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@bloomfountaincoder/question-answering-on-pdf-with-google-palm-2-and-langchain-094ac604d68a?source=author_recirc-----388dcfb1c7e9----0---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Question-Answering on pdf with Google PaLM 2 and Langchain.</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Upload multiple pdf files and ask questions from pdf data using google PaLM 2.</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>7 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Nov 13, 2023</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F094ac604d68a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Fquestion-answering-on-pdf-with-google-palm-2-and-langchain-094ac604d68a&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----094ac604d68a----0-----------------clap_footer----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div><div class="bl" aria-hidden="false" aria-describedby="111" aria-labelledby="111"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="195" aria-labelledby="195"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">4<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="112" aria-labelledby="112"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@bloomfountaincoder/question-answering-on-pdf-with-google-palm-2-and-langchain-094ac604d68a?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----388dcfb1c7e9----0---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="113" aria-labelledby="113"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F094ac604d68a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Fquestion-answering-on-pdf-with-google-palm-2-and-langchain-094ac604d68a&amp;source=-----388dcfb1c7e9----0-----------------bookmark_preview----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@learn-simplified/finally-got-small-company-running-with-100-ai-agents-part-3-34fa2f91b943" tabindex="0"><div class="wt"><div aria-label="Finally Got Small Company Running with 100% AI Agents : Part 3"><div class="wv ww wx wy wz"><img alt="Finally Got Small Company Running with 100% AI Agents : Part 3" class="bg xa xb xc xd bw" src="./Tutorial_files/1_FuEewmd1SMwtINSweCjVrw.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="114" aria-labelledby="114"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@learn-simplified?source=author_recirc-----388dcfb1c7e9----1---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div class="l fi"><img alt="Aniket Hingane" class="l fc bx xo xp cw" src="./Tutorial_files/1_UodjGlnjnRu8Nc0aV6Q5wA.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="115" aria-labelledby="115"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@learn-simplified?source=author_recirc-----388dcfb1c7e9----1---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Aniket Hingane</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@learn-simplified/finally-got-small-company-running-with-100-ai-agents-part-3-34fa2f91b943?source=author_recirc-----388dcfb1c7e9----1---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Finally Got Small Company Running with 100% AI Agents&nbsp;: Part 3</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Witness How to Build a Business with All AI Employees</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><div class="rj ab"><div class="bl" aria-hidden="false" aria-describedby="116" aria-labelledby="116"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="117" aria-labelledby="117"><svg width="16" height="16" viewBox="0 0 64 64" fill="none"><path d="M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>19 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>May 15, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F34fa2f91b943&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40learn-simplified%2Ffinally-got-small-company-running-with-100-ai-agents-part-3-34fa2f91b943&amp;user=Aniket+Hingane&amp;userId=eda76219b00a&amp;source=-----34fa2f91b943----1-----------------clap_footer----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div><div class="bl" aria-hidden="false" aria-describedby="118" aria-labelledby="118"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="197" aria-labelledby="197"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">1.3K<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="119" aria-labelledby="119"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@learn-simplified/finally-got-small-company-running-with-100-ai-agents-part-3-34fa2f91b943?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----388dcfb1c7e9----1---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">15</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="120" aria-labelledby="120"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34fa2f91b943&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40learn-simplified%2Ffinally-got-small-company-running-with-100-ai-agents-part-3-34fa2f91b943&amp;source=-----388dcfb1c7e9----1-----------------bookmark_preview----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@learn-simplified/how-can-we-build-a-small-startup-using-ai-agent-part-1-7e71d057dd4f" tabindex="0"><div class="wt"><div aria-label="How Can we Build a Small Startup using AI Agent : Part 1"><div class="wv ww wx wy wz"><img alt="How Can we Build a Small Startup using AI Agent : Part 1" class="bg xa xb xc xd bw" src="./Tutorial_files/1_LbwTMag07tQHxzLVi1DX4Q.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="121" aria-labelledby="121"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@learn-simplified?source=author_recirc-----388dcfb1c7e9----2---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div class="l fi"><img alt="Aniket Hingane" class="l fc bx xo xp cw" src="./Tutorial_files/1_UodjGlnjnRu8Nc0aV6Q5wA.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="122" aria-labelledby="122"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@learn-simplified?source=author_recirc-----388dcfb1c7e9----2---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Aniket Hingane</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@learn-simplified/how-can-we-build-a-small-startup-using-ai-agent-part-1-7e71d057dd4f?source=author_recirc-----388dcfb1c7e9----2---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">How Can we Build a Small Startup using AI Agent&nbsp;: Part 1</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">A Complete Guide&nbsp;: Every detailed Spelled Out.</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><div class="rj ab"><div class="bl" aria-hidden="false" aria-describedby="123" aria-labelledby="123"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="124" aria-labelledby="124"><svg width="16" height="16" viewBox="0 0 64 64" fill="none"><path d="M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>9 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>May 8, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7e71d057dd4f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40learn-simplified%2Fhow-can-we-build-a-small-startup-using-ai-agent-part-1-7e71d057dd4f&amp;user=Aniket+Hingane&amp;userId=eda76219b00a&amp;source=-----7e71d057dd4f----2-----------------clap_footer----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div><div class="bl" aria-hidden="false" aria-describedby="125" aria-labelledby="125"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="199" aria-labelledby="199"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">391<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="126" aria-labelledby="126"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@learn-simplified/how-can-we-build-a-small-startup-using-ai-agent-part-1-7e71d057dd4f?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----388dcfb1c7e9----2---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">8</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="127" aria-labelledby="127"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e71d057dd4f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40learn-simplified%2Fhow-can-we-build-a-small-startup-using-ai-agent-part-1-7e71d057dd4f&amp;source=-----388dcfb1c7e9----2-----------------bookmark_preview----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@bloomfountaincoder/train-a-custom-named-entity-recognition-model-using-spacy-396f23558ab8" tabindex="0"><div class="wt"><div aria-label="Picture of words"><div class="wv ww wx wy wz"><img alt="Picture of words" class="bg xa xb xc xd bw" src="./Tutorial_files/1_2-ehZEfvaExslL037EapkA.jpg" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="128" aria-labelledby="128"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=author_recirc-----388dcfb1c7e9----3---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div class="l fi"><img alt="Hidiat Ibrahim" class="l fc bx xo xp cw" src="./Tutorial_files/0_SuNGmT-nSEiriMxS(2).png" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="129" aria-labelledby="129"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=author_recirc-----388dcfb1c7e9----3---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Hidiat Ibrahim</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@bloomfountaincoder/train-a-custom-named-entity-recognition-model-using-spacy-396f23558ab8?source=author_recirc-----388dcfb1c7e9----3---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Train a Custom Named Entity Recognition Model Using spaCy</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Train a named entity recognition with custom annotated data.</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>6 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Mar 9, 2023</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi acm kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F396f23558ab8&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ftrain-a-custom-named-entity-recognition-model-using-spacy-396f23558ab8&amp;user=Hidiat+Ibrahim&amp;userId=a22196dac4ca&amp;source=-----396f23558ab8----3-----------------clap_footer----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><div><div class="bl" aria-hidden="false" aria-describedby="130" aria-labelledby="130"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="131" aria-labelledby="131"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@bloomfountaincoder/train-a-custom-named-entity-recognition-model-using-spacy-396f23558ab8?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----388dcfb1c7e9----3---------------------59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="132" aria-labelledby="132"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F396f23558ab8&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bloomfountaincoder%2Ftrain-a-custom-named-entity-recognition-model-using-spacy-396f23558ab8&amp;source=-----388dcfb1c7e9----3-----------------bookmark_preview----59c40c24_fccd_43c2_b583_8c59d58cb018-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class="sw bg sx dj dk zx zy zz"></div><div class="ab ix iy aba abb abc"><a class="be b bf z bj qw abd abe abf ll li abg eu ev ew abh abi abj ez abk abl abm abn abo fa fb fc bl fd fe" rel="noopener follow" href="https://medium.com/@bloomfountaincoder?source=post_page-----388dcfb1c7e9--------------------------------"><div class="l fe">See all from Hidiat Ibrahim</div></a></div></div></div><div class="sw bg sx abp abq abr abs abt"></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="abu abv l"><h2 class="be ss mo mq mr ms mu mv mw my mz na nc nd ne ng nh bj">Recommended from Medium</h2><div class="nm nn no np nq l"><div class="uq ab ki iz ur us ut uu uv uw ux uy uz va vb vc vd ve vf"><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/ubiai-nlp/fine-tuning-gemma-7b-llm-with-hugging-face-e84b45fc8b39" tabindex="0"><div class="wt"><div aria-label="Fine-tuning Gemma 7B LLM with Hugging Face"><div class="wv ww wx wy wz"><img alt="Fine-tuning Gemma 7B LLM with Hugging Face" class="bg xa xb xc xd bw" src="./Tutorial_files/1_dncd5a1MVzgOKBVGCHETsA.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="133" aria-labelledby="133"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@wiem.souai?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Wiem Souai" class="l fc bx xo xp cw" src="./Tutorial_files/1_rqZI483DOJuO3WnH9qIlJw.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="134" aria-labelledby="134"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@wiem.souai?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Wiem Souai</p></a></div></div></div><div class="xq l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="135" aria-labelledby="135"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" href="https://medium.com/ubiai-nlp?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------" rel="noopener follow"><p class="be b du z xr xs xt xu xv xw xx xy bj">UBIAI NLP</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/ubiai-nlp/fine-tuning-gemma-7b-llm-with-hugging-face-e84b45fc8b39?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Fine-tuning Gemma 7B LLM with Hugging Face</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">In the ever-evolving field of artificial intelligence, Google presents Gemma, an advanced open-source language model designed for a wide…</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>8 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Mar 8, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fubiai-nlp%2Fe84b45fc8b39&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fubiai-nlp%2Ffine-tuning-gemma-7b-llm-with-hugging-face-e84b45fc8b39&amp;user=Wiem+Souai&amp;userId=7f5b0b3bae8&amp;source=-----e84b45fc8b39----0-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="136" aria-labelledby="136"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="202" aria-labelledby="202"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">40<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="137" aria-labelledby="137"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/ubiai-nlp/fine-tuning-gemma-7b-llm-with-hugging-face-e84b45fc8b39?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">1</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="138" aria-labelledby="138"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe84b45fc8b39&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fubiai-nlp%2Ffine-tuning-gemma-7b-llm-with-hugging-face-e84b45fc8b39&amp;source=-----388dcfb1c7e9----0-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@dassum/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07" tabindex="0"><div class="wt"><div aria-label="Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA"><div class="wv ww wx wy wz"><img alt="Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA" class="bg xa xb xc xd bw" src="./Tutorial_files/1_3H2V5GJOfaj3SGt1lcibfA.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="139" aria-labelledby="139"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@dassum?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Suman Das" class="l fc bx xo xp cw" src="./Tutorial_files/1_d3CoFf8bJpFnBeJXUp-gKg.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="140" aria-labelledby="140"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@dassum?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Suman Das</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@dassum/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">The field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and…</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>15 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Jan 25, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ffb60abdeba07&amp;operation=register&amp;redirect=https%3A%2F%2Fdassum.medium.com%2Ffine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07&amp;user=Suman+Das&amp;userId=54106b197f0a&amp;source=-----fb60abdeba07----1-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="141" aria-labelledby="141"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="204" aria-labelledby="204"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">1.2K<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="142" aria-labelledby="142"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@dassum/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">16</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="143" aria-labelledby="143"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffb60abdeba07&amp;operation=register&amp;redirect=https%3A%2F%2Fdassum.medium.com%2Ffine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07&amp;source=-----388dcfb1c7e9----1-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div></div><div class="sw bg sx abw"></div><h2 class="be ss in z gs bj">Lists</h2><div class="zw l"><div class="cm ab ki iz ur us ut uu uv uw ux uy uz va vb vc vd ve vf"><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" rel="noopener follow" href="https://medium.com/@ben.putney/list/predictive-modeling-w-python-e3668ea008e1?source=read_next_recirc-----388dcfb1c7e9--------------------------------"><div class="acc acd xr ab iw fi"><div class="fi xa aby bw abz"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/0_r4yjMpEmqzHCUvWC.jpg" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi xa aby bw kj aca"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/1_bv2KUVNLi2sFNjBTdoBmWw.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi xa bw rl acb"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/0_zsngbTOmFCy6sUCx.jpg" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be ss in z xr zc xt xu zd xw xy gs bj">Predictive Modeling w/ Python</h2><div class="be b du z dt ab abx">20 stories<span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span>1243 saves</div></div></a></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=read_next_recirc-----388dcfb1c7e9--------------------------------" rel="noopener follow"><div class="acc acd xr ab iw fi"><div class="fi xa aby bw abz"><div class="xa ig xr l"><img alt="Principal Component Analysis for ML" class="" src="./Tutorial_files/1_swd_PY6vTCyPnsgBYoFZfA.png" width="48" height="48" loading="lazy"></div></div><div class="fi xa aby bw kj aca"><div class="xa ig xr l"><img alt="Time Series Analysis" class="" src="./Tutorial_files/1_8sSAHftNwd_RNJ3k4VA0pA.png" width="48" height="48" loading="lazy"></div></div><div class="fi xa bw rl acb"><div class="xa ig xr l"><img alt="deep learning cheatsheet for beginner" class="" src="./Tutorial_files/1_uNyD4yNMH-DnOel1wzxOOA.png" width="48" height="48" loading="lazy"></div></div></div><div class="aw l"><h2 class="be ss in z xr zc xt xu zd xw xy gs bj">Practical Guides to Machine Learning</h2><div class="be b du z dt ab abx">10 stories<span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span>1500 saves</div></div></a></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" rel="noopener follow" href="https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----388dcfb1c7e9--------------------------------"><div class="acc acd xr ab iw fi"><div class="fi xa aby bw abz"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/1_F708LqxtD0Nm3C0UOd8Qdw.jpg" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi xa aby bw kj aca"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/0_XlqkUK1qiERGWmg9.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi xa bw rl acb"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/1_IbT9NbQzFr8x9GZxubQQfg.png" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be ss in z xr zc xt xu zd xw xy gs bj">Natural Language Processing</h2><div class="be b du z dt ab abx">1488 stories<span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span>999 saves</div></div></a></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" rel="noopener follow" href="https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=read_next_recirc-----388dcfb1c7e9--------------------------------"><div class="acc acd xr ab iw fi"><div class="fi xa aby bw abz"><div class="xa ig xr l"><img alt="Image by vectorjuice on FreePik" class="" src="./Tutorial_files/0_3OsUtsnlTx9Svm4c.jpg" width="48" height="48" loading="lazy"></div></div><div class="fi xa aby bw kj aca"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/1_IPZF1hcDWwpPqOz2vL7NxQ.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi xa bw rl acb"><div class="xa ig xr l"><img alt="" class="" src="./Tutorial_files/1_0fHUKyg3xtpNWpop35PR4g.png" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be ss in z xr zc xt xu zd xw xy gs bj">The New Chatbots: ChatGPT, Bard, and Beyond</h2><div class="be b du z dt ab abx">12 stories<span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span>390 saves</div></div></a></div></div></div><div class="sw bg sx ace dj acf dk acg ach aci acj ack acl"></div><div class="uq ab ki iz ur us ut uu uv uw ux uy uz va vb vc vd ve vf"><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@srishti.gupta_37992/fine-tuning-llms-for-healthcare-data-4c48e7f62d62" tabindex="0"><div class="wt"><div aria-label="Fine Tuning LLMs for Healthcare Data"><div class="wv ww wx wy wz"><img alt="Fine Tuning LLMs for Healthcare Data" class="bg xa xb xc xd bw" src="./Tutorial_files/1_JcEXSVgFHrlWUQyNkNYP9g.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="144" aria-labelledby="144"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@srishti.gupta_37992?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Srishti Gupta" class="l fc bx xo xp cw" src="./Tutorial_files/0_xBMJ0QYRmvTLbf33.png" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="145" aria-labelledby="145"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@srishti.gupta_37992?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Srishti Gupta</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@srishti.gupta_37992/fine-tuning-llms-for-healthcare-data-4c48e7f62d62?source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Fine Tuning LLMs for Healthcare Data</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Authors: Srishti Gupta, Daniel Lievano, Raffaele Mannarelli, Nishant Kushwaha, Abhinav Sharma</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>7 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Mar 18, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4c48e7f62d62&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40srishti.gupta_37992%2Ffine-tuning-llms-for-healthcare-data-4c48e7f62d62&amp;user=Srishti+Gupta&amp;userId=d02ca14c3da7&amp;source=-----4c48e7f62d62----0-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="146" aria-labelledby="146"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="206" aria-labelledby="206"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">8<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="147" aria-labelledby="147"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@srishti.gupta_37992/fine-tuning-llms-for-healthcare-data-4c48e7f62d62?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----0---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="148" aria-labelledby="148"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c48e7f62d62&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40srishti.gupta_37992%2Ffine-tuning-llms-for-healthcare-data-4c48e7f62d62&amp;source=-----388dcfb1c7e9----0-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@shitalnandre108/initiating-gemma-fine-tuning-on-google-colab-a-comprehensive-guide-b9006f9da138" tabindex="0"><div class="wt"><div aria-label="Initiating Gemma Fine-Tuning on Google Colab: A Comprehensive Guide"><div class="wv ww wx wy wz"><img alt="Initiating Gemma Fine-Tuning on Google Colab: A Comprehensive Guide" class="bg xa xb xc xd bw" src="./Tutorial_files/1_7eKokCNeUMK1Zo3ciwLsXw.jpg" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="149" aria-labelledby="149"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@shitalnandre108?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Shital Nandre" class="l fc bx xo xp cw" src="./Tutorial_files/1_0SsVyPYaUm9qZUS0mFBjEw.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="150" aria-labelledby="150"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@shitalnandre108?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Shital Nandre</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@shitalnandre108/initiating-gemma-fine-tuning-on-google-colab-a-comprehensive-guide-b9006f9da138?source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Initiating Gemma Fine-Tuning on Google Colab: A Comprehensive Guide</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Unlock the potential of GEMMA, Google’s cutting-edge language model, with this comprehensive tutorial on fine-tuning. Discover how to…</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>6 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Feb 25, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fb9006f9da138&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40shitalnandre108%2Finitiating-gemma-fine-tuning-on-google-colab-a-comprehensive-guide-b9006f9da138&amp;user=Shital+Nandre&amp;userId=b3af33166139&amp;source=-----b9006f9da138----1-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="151" aria-labelledby="151"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="208" aria-labelledby="208"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">8<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="152" aria-labelledby="152"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@shitalnandre108/initiating-gemma-fine-tuning-on-google-colab-a-comprehensive-guide-b9006f9da138?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----1---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="153" aria-labelledby="153"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9006f9da138&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40shitalnandre108%2Finitiating-gemma-fine-tuning-on-google-colab-a-comprehensive-guide-b9006f9da138&amp;source=-----388dcfb1c7e9----1-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@achillesmoraites/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19" tabindex="0"><div class="wt"><div aria-label="Lightweight RoBERTa Sequence Classification Fine-Tuning with LORA using the Hugging Face PEFT…"><div class="wv ww wx wy wz"><img alt="Lightweight RoBERTa Sequence Classification Fine-Tuning with LORA using the Hugging Face PEFT…" class="bg xa xb xc xd bw" src="./Tutorial_files/1_GBPVsi3-hfya0CLXZrNBzw.jpg" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="154" aria-labelledby="154"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@achillesmoraites?source=read_next_recirc-----388dcfb1c7e9----2---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Achilles Moraites" class="l fc bx xo xp cw" src="./Tutorial_files/1_2SJzqS06DSovGN1L1ZCqoA.jpg" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="155" aria-labelledby="155"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@achillesmoraites?source=read_next_recirc-----388dcfb1c7e9----2---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Achilles Moraites</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@achillesmoraites/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19?source=read_next_recirc-----388dcfb1c7e9----2---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title="Lightweight RoBERTa Sequence Classification Fine-Tuning with LORA using the Hugging Face PEFT…"><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Lightweight RoBERTa Sequence Classification Fine-Tuning with LORA using the Hugging Face PEFT…</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Fine-tuning large language models (LLMs) like RoBERTa can produce remarkable results when adapting them to specific tasks.&nbsp;
Unfortunately…</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>4 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Feb 11, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F8dd9edf99d19&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40achillesmoraites%2Flightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19&amp;user=Achilles+Moraites&amp;userId=7112cb79ae68&amp;source=-----8dd9edf99d19----2-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="156" aria-labelledby="156"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="210" aria-labelledby="210"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">157<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="157" aria-labelledby="157"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@achillesmoraites/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----2---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">1</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="158" aria-labelledby="158"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8dd9edf99d19&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40achillesmoraites%2Flightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19&amp;source=-----388dcfb1c7e9----2-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div><div class="j i d"><div class="sw bg sx zw"></div></div></div></div></div></div></div></div></article></div></div><div class="vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa"><div class="wb wc wd we wf dv l"><article class="dv"><div class="dv rj l"><div class="bg dv"><div class="dv l"><div class="fi dv wg wh wi wj wk wl wm wn wo wp wq wr ws" role="link" data-href="https://medium.com/@nabilw/fine-tuning-llama-3-on-mental-health-dataset-70d4e69b8875" tabindex="0"><div class="wt"><div aria-label="Fine-tuning LLaMA 3 on Mental Health Dataset"><div class="wv ww wx wy wz"><img alt="Fine-tuning LLaMA 3 on Mental Health Dataset" class="bg xa xb xc xd bw" src="./Tutorial_files/1_zq95vCwJVqkb8sTspM0IDQ.png" loading="lazy"></div></div></div><div class="wu ab ca cn"><div class="ab cn xe bg xf xg xh xi"><div class="xj xk xl xm xn ab q"><div class="qv l"><div><div class="l" aria-hidden="false" aria-describedby="159" aria-labelledby="159"><a tabindex="-1" rel="noopener follow" href="https://medium.com/@nabilw?source=read_next_recirc-----388dcfb1c7e9----3---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div class="l fi"><img alt="Nabil Wasti" class="l fc bx xo xp cw" src="./Tutorial_files/1_L8FQQEK2t0gn0XmU-WpLSw.png" width="20" height="20" loading="lazy"><div class="fq bx l xo xp fr n ax fs"></div></div></a></div></div></div><div class="xq l"><div><div class="l" aria-hidden="false" aria-describedby="160" aria-labelledby="160"><a class="af ag ah ai aj ak al am an ao ap aq ar ip ab q" rel="noopener follow" href="https://medium.com/@nabilw?source=read_next_recirc-----388dcfb1c7e9----3---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><p class="be b du z xr xs xt xu xv xw xx xy bj">Nabil Wasti</p></a></div></div></div></div><div class="xz ya yb yc yd ye yf yg yh yi l gm"><div class="yj yk yl ym yn yo yp yq"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/@nabilw/fine-tuning-llama-3-on-mental-health-dataset-70d4e69b8875?source=read_next_recirc-----388dcfb1c7e9----3---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div title=""><h2 class="be gu mo mq yr ys mr ms mu yt yu mv on po yv yw pp or pr yx yy ps ov pu yz za pv xr xt xu xw xy bj">Fine-tuning LLaMA 3 on Mental Health Dataset</h2></div><div class="zb l"><h3 class="be b in z xr zc xt xu zd xw xy dt">Part 1: Introduction</h3></div></a></div></div><span class="be b du z dt"><div class="ab q"><span>12 min read</span><span class="iq l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span>Apr 21, 2024</span></div></span><div class="ze zf zg zh zi l"><div class="ab co"><div class="am zj zk zl zm zn zo zp zq zr zs ab q"><div class="ab q ki kj"><div class="pw-multi-vote-icon fi kk kl km kn"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F70d4e69b8875&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40nabilw%2Ffine-tuning-llama-3-on-mental-health-dataset-70d4e69b8875&amp;user=Nabil+Wasti&amp;userId=b4081a5b616e&amp;source=-----70d4e69b8875----3-----------------clap_footer----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><div><div class="bl" aria-hidden="false" aria-describedby="161" aria-labelledby="161"><div class="ko ao kp kq kr ks am kt ku kv kn"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><div><div class="bl" aria-hidden="false" aria-describedby="212" aria-labelledby="212"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at ul ll">73<span class="l h g f rr rs"></span></button></p></div></div></div></div><div class="zt l"><div><div class="bl" aria-hidden="false" aria-describedby="162" aria-labelledby="162"><a class="af fj ah ko aj ak al lh an ao ap aq ar as at lg ab q li lj" aria-label="responses" rel="noopener follow" href="https://medium.com/@nabilw/fine-tuning-llama-3-on-mental-health-dataset-70d4e69b8875?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----388dcfb1c7e9----3---------------------42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b du z dt"><span class="pw-responses-count le lf">4</span></p></a></div></div></div></div><div class="ab q zu zv"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="163" aria-labelledby="163"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70d4e69b8875&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40nabilw%2Ffine-tuning-llama-3-on-mental-health-dataset-70d4e69b8875&amp;source=-----388dcfb1c7e9----3-----------------bookmark_preview----42507151_447d_42a1_a8a5_c0c9f868e92f-------"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="dt ll" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="currentColor"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class="sw bg sx dj dk zx zy zz"></div><a class="be b bf z bj qw abd abe abf ll li abg eu ev ew abh abi abj ez abk abl abm abn abo fa fb fc bl fd fe" rel="noopener follow" href="https://medium.com/?source=post_page-----388dcfb1c7e9--------------------------------"><div class="l fe">See more recommendations</div></a></div></div></div><div class="h k j"><div class="sw bg sx td"></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="te ab ki iz"><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Help</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Status</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/about?autoplay=1&amp;source=post_page-----388dcfb1c7e9--------------------------------"><p class="be b du z dt">About</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----388dcfb1c7e9--------------------------------"><p class="be b du z dt">Careers</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="mailto:pressinquiries@medium.com?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Press</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Blog</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Privacy</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Terms</p></a></div><div class="tf tg l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----388dcfb1c7e9--------------------------------" rel="noopener follow"><p class="be b du z dt">Text to speech</p></a></div><div class="tf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://medium.com/business?source=post_page-----388dcfb1c7e9--------------------------------"><p class="be b du z dt">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20240530-222836-0da94305aa"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-388dcfb1c7e9","user-a22196dac4ca"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[],"updatedPostPreviewsEnabled":false,"lohpExperimentEnabled":"group_1"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"DEFAULT","explicit":false},"viewerIsBot":false},"debug":{"requestId":"1bdc3bca-ead6-4414-aae9-af63e7016f6b","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"4714184b7d57bf52","ot-tracer-traceid":"53039b37608f94aa","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002F@bloomfountaincoder\u002Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9","host":"medium.com","hostname":"medium.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"queryString":"","currentHash":""},"config":{"nodeEnv":"production","version":"main-20240530-222836-0da94305aa","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20240530-222836-0da94305aa","commit":"0da94305aa445205112d0db544c14170fb792385"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","viewer":null,"isLoggedIn":false,"collectionByDomainOrSlug({\"domainOrSlug\":\"medium.com\"})":null,"postResult({\"id\":\"388dcfb1c7e9\"})":{"__ref":"Post:388dcfb1c7e9"}},"LinkedAccounts:a22196dac4ca":{"__typename":"LinkedAccounts","mastodon":null,"id":"a22196dac4ca"},"UserViewerEdge:userId:a22196dac4ca-viewerId:lo_9d13371ea304":{"__typename":"UserViewerEdge","id":"userId:a22196dac4ca-viewerId:lo_9d13371ea304","isFollowing":false,"isUser":false,"isMuting":false},"User:a22196dac4ca":{"__typename":"User","id":"a22196dac4ca","linkedAccounts":{"__ref":"LinkedAccounts:a22196dac4ca"},"isSuspended":false,"imageId":"0*SuNGmT-nSEiriMxS","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"name":"Hidiat Ibrahim","socialStats":{"__typename":"SocialStats","followerCount":1},"username":"bloomfountaincoder","customDomainState":null,"hasSubdomain":false,"bio":"I develop machine learning, NLP, LLM, and computer vision apps. Certified AWS ML. Freelancer at Upwork. https:\u002F\u002Fwww.upwork.com\u002Ffreelancers\u002F~0124f2a56984ab4a4b","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:a22196dac4ca-viewerId:lo_9d13371ea304"},"viewerIsUser":false,"newsletterV3":null,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":null,"twitterScreenName":""},"Paragraph:435b79f14100_0":{"__typename":"Paragraph","id":"435b79f14100_0","name":"03e3","type":"H3","href":null,"layout":null,"metadata":null,"text":"Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_1":{"__typename":"Paragraph","id":"435b79f14100_1","name":"428c","type":"H3","href":null,"layout":null,"metadata":null,"text":"Fine-tune Falcon 7B LLM.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*9PSSJMGe60zzIkKE":{"__typename":"ImageMetadata","id":"0*9PSSJMGe60zzIkKE","originalHeight":2121,"originalWidth":2947,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:435b79f14100_2":{"__typename":"Paragraph","id":"435b79f14100_2","name":"ce29","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:0*9PSSJMGe60zzIkKE"},"text":"Photo by Stephen Dawson on Unsplash","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":9,"end":23,"href":"https:\u002F\u002Funsplash.com\u002F@dawson2406?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":27,"end":35,"href":"https:\u002F\u002Funsplash.com?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_3":{"__typename":"Paragraph","id":"435b79f14100_3","name":"1679","type":"P","href":null,"layout":null,"metadata":null,"text":"PEFT \u002F LoRA \u002F QLoRA","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_4":{"__typename":"Paragraph","id":"435b79f14100_4","name":"5357","type":"P","href":null,"layout":null,"metadata":null,"text":"Large language models (LLMs) typically have huge parameters running from hundreds of millions to billions due to the huge datasets they were trained on and several layers of their transformer model architecture. This makes fine-tuning all the layers of the model computationally prohibitive. There have been several efforts to find a way to fine-tune LLMs without the high computation costs. This gave rise to several methods such as parameter-efficient fine-tuning. Parameter-efficient fine-tuning (PEFT) comprises different techniques such as prompt-tuning, prefix-tuning, p-tuning, low-rank adaptation, and quantization with low-rank adaptation(QLoRA).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_5":{"__typename":"Paragraph","id":"435b79f14100_5","name":"5251","type":"P","href":null,"layout":null,"metadata":null,"text":"LoRA (Low-Rank Adaptation) is a reparameterization method that decomposes the weight change matrix of an LLM into low-rank matrices. These low-rank matrices are typically inserted in the attention blocks of the model. The original weight matrix of the pre-trained model is frozen and only the inserted smaller matrices are updated during training. This reduces the number of trainable parameters, reducing memory usage and training time which can be very expensive for large models.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_6":{"__typename":"Paragraph","id":"435b79f14100_6","name":"daec","type":"P","href":null,"layout":null,"metadata":null,"text":"In this notebook, we’ll fine-tune Falcon LLM 7b with QLoRA. QLoRA adds quantization to LoRA by loading the base model in 4-bit floating point precision before applying LoRA. This reduces the memory consumption of LLM fine-tuning without a significant reduction in performance. Quantization is a technique in which the floating point precision of the parameters is reduced from 32 bits to up to 4 bits, without losing a lot of information.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_7":{"__typename":"Paragraph","id":"435b79f14100_7","name":"6613","type":"P","href":null,"layout":null,"metadata":null,"text":"Falcon-7B is a 7B parameters causal decoder-only model that was trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. FalconLLM was developed by the Technology Innovation Institute, UAE. Falcon LLM is open-source and it’s available on the Hugging Face website.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":92,"end":102,"href":"https:\u002F\u002Fhuggingface.co\u002Fdatasets\u002Ftiiuae\u002Ffalcon-refinedweb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_8":{"__typename":"Paragraph","id":"435b79f14100_8","name":"2e82","type":"P","href":null,"layout":null,"metadata":null,"text":"We are going to run this notebook on the free tier T4 GPU on Google Colab. This compute resource is still insufficient for proper fine-tuning of LLM and because of the limited compute resources, we’ll use a sharded version of Falcon LLM available on Hugging Face contributed by Younes Belkada.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":207,"end":222,"href":"https:\u002F\u002Fhuggingface.co\u002Fybelkada\u002Ffalcon-7b-sharded-bf16\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":278,"end":292,"href":"https:\u002F\u002Fhuggingface.co\u002Fybelkada","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_9":{"__typename":"Paragraph","id":"435b79f14100_9","name":"c4c3","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll follow these steps:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_10":{"__typename":"Paragraph","id":"435b79f14100_10","name":"60d9","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Install and load libraries and dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_11":{"__typename":"Paragraph","id":"435b79f14100_11","name":"c30b","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Load the base model with quantization (for QLoRA)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_12":{"__typename":"Paragraph","id":"435b79f14100_12","name":"9885","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Configure LoRA adapter","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_13":{"__typename":"Paragraph","id":"435b79f14100_13","name":"c858","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Set up training arguments","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_14":{"__typename":"Paragraph","id":"435b79f14100_14","name":"7fdf","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Train the model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_15":{"__typename":"Paragraph","id":"435b79f14100_15","name":"47cb","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Save the model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_16":{"__typename":"Paragraph","id":"435b79f14100_16","name":"3469","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Create generation config for prediction","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_17":{"__typename":"Paragraph","id":"435b79f14100_17","name":"1f28","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Make inferences on sample test data","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_18":{"__typename":"Paragraph","id":"435b79f14100_18","name":"f8dd","type":"H4","href":null,"layout":null,"metadata":null,"text":"Libraries","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_19":{"__typename":"Paragraph","id":"435b79f14100_19","name":"eec3","type":"P","href":null,"layout":null,"metadata":null,"text":"We need to import some libraries to implement QLoRA.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_20":{"__typename":"Paragraph","id":"435b79f14100_20","name":"a84b","type":"P","href":null,"layout":null,"metadata":null,"text":"1. Transformer Reinforcement Learning (trl). This helps us train the language model with reinforcement learning. It’s integrated with huggingface transformers. TRL supports decoder models such as GPT-2, BLOOM, and Falcon LLM.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_21":{"__typename":"Paragraph","id":"435b79f14100_21","name":"f4d4","type":"P","href":null,"layout":null,"metadata":null,"text":"2. PEFT. This library is used for efficiently adapting pre-trained language models by fine-tuning only a small number of model parameters significantly reducing computational and storage costs.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_22":{"__typename":"Paragraph","id":"435b79f14100_22","name":"2c35","type":"P","href":null,"layout":null,"metadata":null,"text":"3. Sharded Falcon LLM. Available on huggingface. The sharded falcon model loads faster on low compute than the original model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_23":{"__typename":"Paragraph","id":"435b79f14100_23","name":"6814","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fybelkada\u002Ffalcon-7b-sharded-bf16\u002Ftree\u002Fmain","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":64,"href":"https:\u002F\u002Fhuggingface.co\u002Fybelkada\u002Ffalcon-7b-sharded-bf16\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_24":{"__typename":"Paragraph","id":"435b79f14100_24","name":"89a1","type":"P","href":null,"layout":null,"metadata":null,"text":"4. Accelerate makes the Pytorch training loop faster.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_25":{"__typename":"Paragraph","id":"435b79f14100_25","name":"7d0e","type":"P","href":null,"layout":null,"metadata":null,"text":"5. Bitsandbytes is a lightweight wrapper around CUDA custom functions, particularly 8-bit optimizers and quantization functions. It’s used to handle the quantization process in QLoRA. It’s developed by HuggingFace.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_26":{"__typename":"Paragraph","id":"435b79f14100_26","name":"6385","type":"P","href":null,"layout":null,"metadata":null,"text":"6. Einops simplifies tensor operations.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_27":{"__typename":"Paragraph","id":"435b79f14100_27","name":"bc9c","type":"P","href":null,"layout":null,"metadata":null,"text":"7. Datasets makes it easy to load datasets from Huggingface datasets repository.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_28":{"__typename":"Paragraph","id":"435b79f14100_28","name":"2663","type":"P","href":null,"layout":null,"metadata":null,"text":"8. Transformers is the standard Huggingface library for accessing pre-trained models on Huggingface using python.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_29":{"__typename":"Paragraph","id":"435b79f14100_29","name":"982e","type":"PRE","href":null,"layout":null,"metadata":null,"text":"!pip install -q trl transformers accelerate peft datasets bitsandbytes einops","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_30":{"__typename":"Paragraph","id":"435b79f14100_30","name":"a3d5","type":"H3","href":null,"layout":null,"metadata":null,"text":"Import the libraries","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_31":{"__typename":"Paragraph","id":"435b79f14100_31","name":"b185","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import os\nimport torch\nimport transformers\nimport pandas as pd\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel, PeftConfig\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom peft import  prepare_model_for_kbit_training, get_peft_model, TaskType\n\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n    pipeline,\n    logging\n    )","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_32":{"__typename":"Paragraph","id":"435b79f14100_32","name":"764d","type":"H3","href":null,"layout":null,"metadata":null,"text":"Dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_33":{"__typename":"Paragraph","id":"435b79f14100_33","name":"d326","type":"P","href":null,"layout":null,"metadata":null,"text":"E-commerce customer sentiment analysis data. Data","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":45,"end":49,"href":"https:\u002F\u002Fhuggingface.co\u002Fdatasets\u002Farize-ai\u002Fecommerce_reviews_with_language_drift?row=13","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_34":{"__typename":"Paragraph","id":"435b79f14100_34","name":"0dfd","type":"P","href":null,"layout":null,"metadata":null,"text":"Load the dataset and convert it into a dataframe so that we can work on it with pandas and scikit-learn.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_35":{"__typename":"Paragraph","id":"435b79f14100_35","name":"4d24","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data = load_dataset('arize-ai\u002Fecommerce_reviews_with_language_drift', split='validation')","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_36":{"__typename":"Paragraph","id":"435b79f14100_36","name":"3903","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data = pd.DataFrame(data)\ndata.head(10)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_37":{"__typename":"Paragraph","id":"435b79f14100_37","name":"015d","type":"P","href":null,"layout":null,"metadata":null,"text":"Extracting the two columns we need out of the dataframe.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_38":{"__typename":"Paragraph","id":"435b79f14100_38","name":"95c2","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data = data[['text', 'label']]\ndata.head()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_39":{"__typename":"Paragraph","id":"435b79f14100_39","name":"7dcb","type":"P","href":null,"layout":null,"metadata":null,"text":"Convert the label from number to text.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_40":{"__typename":"Paragraph","id":"435b79f14100_40","name":"bb57","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data['label'] = data['label'].replace([0, 1, 2], ['negative', 'neutral', 'positive'])","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_41":{"__typename":"Paragraph","id":"435b79f14100_41","name":"1ce8","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll format the data to include both the text input and the expected output and then, train LLM with the new combination. The LLM will learn from this combined data.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_42":{"__typename":"Paragraph","id":"435b79f14100_42","name":"7987","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data['formatted_data'] = data.apply(lambda row: str(row['text']) + \" -\u003E: \" + row['label'], axis = 1)\ndata.head()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*k6t0LnKfJ77sUIHBGcjdSQ.png":{"__typename":"ImageMetadata","id":"1*k6t0LnKfJ77sUIHBGcjdSQ.png","originalHeight":272,"originalWidth":894,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:435b79f14100_43":{"__typename":"Paragraph","id":"435b79f14100_43","name":"5551","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*k6t0LnKfJ77sUIHBGcjdSQ.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_44":{"__typename":"Paragraph","id":"435b79f14100_44","name":"046b","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s view sample data from the new column.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_45":{"__typename":"Paragraph","id":"435b79f14100_45","name":"8ea0","type":"PRE","href":null,"layout":null,"metadata":null,"text":"data['formatted_data'][0]","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TrYUF3cRKxRKSEYTk45M7A.png":{"__typename":"ImageMetadata","id":"1*TrYUF3cRKxRKSEYTk45M7A.png","originalHeight":47,"originalWidth":1498,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:435b79f14100_46":{"__typename":"Paragraph","id":"435b79f14100_46","name":"e25d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*TrYUF3cRKxRKSEYTk45M7A.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_47":{"__typename":"Paragraph","id":"435b79f14100_47","name":"1a17","type":"P","href":null,"layout":null,"metadata":null,"text":"We are using only a few data points because we have low computation resources. Let’s split the data into train and test splits.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_48":{"__typename":"Paragraph","id":"435b79f14100_48","name":"f23e","type":"PRE","href":null,"layout":null,"metadata":null,"text":"train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_49":{"__typename":"Paragraph","id":"435b79f14100_49","name":"67e2","type":"P","href":null,"layout":null,"metadata":null,"text":"Hugging Face transformer models expect the training data in DatasetDict format.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_50":{"__typename":"Paragraph","id":"435b79f14100_50","name":"6732","type":"PRE","href":null,"layout":null,"metadata":null,"text":"train_dict = DatasetDict({\n    'train': Dataset.from_pandas(train_df)\n})","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_51":{"__typename":"Paragraph","id":"435b79f14100_51","name":"aada","type":"H3","href":null,"layout":null,"metadata":null,"text":"Load the base model with quantization.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_52":{"__typename":"Paragraph","id":"435b79f14100_52","name":"a318","type":"P","href":null,"layout":null,"metadata":null,"text":"In the code snippet below, we’ll use the bitsandbytes library to quantize the model. Bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication, and quantization functions.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_53":{"__typename":"Paragraph","id":"435b79f14100_53","name":"b3ed","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll set the `bitsandbytes` configuration to load the model in 4-bit.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":15,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_54":{"__typename":"Paragraph","id":"435b79f14100_54","name":"bfa2","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, we’ll load the model with the hugging face class `AutoModelForCausalLM` (for the next text generation) and pass the quantization configuration into the model. We’ll set `trust_remote_code` to True because we are accessing it via huggingface.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":56,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":176,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_55":{"__typename":"Paragraph","id":"435b79f14100_55","name":"a444","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll set `model.config.use_cache` to false because the KV cache is not useful during training (Finetune) since the weights will be updated. The cache is set to true during inference only.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":11,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_56":{"__typename":"Paragraph","id":"435b79f14100_56","name":"2355","type":"P","href":null,"layout":null,"metadata":null,"text":"The `prepare_model_for_kbit_training` function makes the model available for training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":5,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_57":{"__typename":"Paragraph","id":"435b79f14100_57","name":"bbc5","type":"P","href":null,"layout":null,"metadata":null,"text":"`Double quantization` is the process of quantizing the quantization constants used during the quantization process in the 4-bit NF quantization. This can save 0.5 bits per parameter on average, as mentioned in the paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_58":{"__typename":"Paragraph","id":"435b79f14100_58","name":"2393","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll enable gradient checkpointing to reduce the number of stored activations and thus, save memory. However, this leads to a slower backward pass.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_59":{"__typename":"Paragraph","id":"435b79f14100_59","name":"fde6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model_name = \"ybelkada\u002Ffalcon-7b-sharded-bf16\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map={\"\":0}\n)\n\nmodel.gradient_checkpointing_enable()\n# Prepares the model for kbit training\nmodel = prepare_model_for_kbit_training(model)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_60":{"__typename":"Paragraph","id":"435b79f14100_60","name":"bb74","type":"P","href":null,"layout":null,"metadata":null,"text":"Print the number of trainable and total parameters in the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_61":{"__typename":"Paragraph","id":"435b79f14100_61","name":"1d41","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params \u002F all_param}\"\n        )","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_62":{"__typename":"Paragraph","id":"435b79f14100_62","name":"b13c","type":"H3","href":null,"layout":null,"metadata":null,"text":"Configure LoRA Adapter.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_63":{"__typename":"Paragraph","id":"435b79f14100_63","name":"b1c0","type":"P","href":null,"layout":null,"metadata":null,"text":"LoRA uses the adapter technique to add new smaller trainable parameters to the model. In the code snippet below, we’ll configure the LoRA adapter. The LoRA adapter works by reparameterizing the weights of a layer matrix usually the linear layers. For the best performance, we’ll include all linear layers in the target modules.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_64":{"__typename":"Paragraph","id":"435b79f14100_64","name":"eedb","type":"P","href":null,"layout":null,"metadata":null,"text":"`lora_alpha`: This is the scaling factor for the LoRA update matrices. The higher the value of lora_alpha the more aggressive the updates to the weights.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_65":{"__typename":"Paragraph","id":"435b79f14100_65","name":"b5ee","type":"P","href":null,"layout":null,"metadata":null,"text":"`lora_dropout`: This is the dropout percentage for the LoRA layers.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_66":{"__typename":"Paragraph","id":"435b79f14100_66","name":"2bb4","type":"P","href":null,"layout":null,"metadata":null,"text":"`r`: This is the rank of the update matrices. Lower rank results in smaller update matrices with fewer trainable parameters. A higher rank will result in larger matrices, which can hold more information about the weights of the layer matrix.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":1,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_67":{"__typename":"Paragraph","id":"435b79f14100_67","name":"90fe","type":"P","href":null,"layout":null,"metadata":null,"text":"`bias`: This determines whether the bias parameters should be updated during training. The bias parameters are the weights that are added to the output of a layer. The value can be ‘none’, ‘all’, or ‘lora_only’. We’ll choose none to preserve the base model output.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":1,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_68":{"__typename":"Paragraph","id":"435b79f14100_68","name":"d98e","type":"P","href":null,"layout":null,"metadata":null,"text":"`task_type`: This specifies the task type for which the LoRA adapter is being used. We’ll choose “CAUSAL_LM”. Other values for `task_type` could be “NLI” or “MT”","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":128,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_69":{"__typename":"Paragraph","id":"435b79f14100_69","name":"639c","type":"P","href":null,"layout":null,"metadata":null,"text":"`target_modules`: These are the modules to which the LoRA update matrices will be applied. The target modules are the layers in the base model that will be reparameterized by the LoRA adapter. Other values for target_modules could be [“attention”, “dense_final”] or [“query_key_value”, “dense”, “dense_h_to_4h”].","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":1,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_70":{"__typename":"Paragraph","id":"435b79f14100_70","name":"427f","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, we’ll create an object of the LoraConfig class and pass in the selected parameters.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_71":{"__typename":"Paragraph","id":"435b79f14100_71","name":"efa3","type":"PRE","href":null,"layout":null,"metadata":null,"text":"lora_alpha = 32\nlora_dropout = 0.05\nlora_r = 8\n\nlora_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"query_key_value\",\n        \"dense\",\n        \"dense_h_to_4h\",\n        \"dense_4h_to_h\",\n    ])","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_72":{"__typename":"Paragraph","id":"435b79f14100_72","name":"88d8","type":"P","href":null,"layout":null,"metadata":null,"text":"Combine the quantized base model with the LoRA adapter using `get_peft_model` and pass the lora_config along with the pre-trained Falcon base model.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":62,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_73":{"__typename":"Paragraph","id":"435b79f14100_73","name":"50e1","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Now you get a model ready for QLoRA training\nlora_model = get_peft_model(model, lora_config)\nlora_model.print_trainable_parameters()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fDk7Yoj1KZs4x-iVsss6Nw.png":{"__typename":"ImageMetadata","id":"1*fDk7Yoj1KZs4x-iVsss6Nw.png","originalHeight":40,"originalWidth":871,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:435b79f14100_74":{"__typename":"Paragraph","id":"435b79f14100_74","name":"874b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*fDk7Yoj1KZs4x-iVsss6Nw.png"},"text":"Number of trainable and total parameters","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_75":{"__typename":"Paragraph","id":"435b79f14100_75","name":"fd95","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll fine-tune the constructed custom LoRA model using huggingface Trainer API. First, we’ll set the training arguments.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_76":{"__typename":"Paragraph","id":"435b79f14100_76","name":"a6b4","type":"H3","href":null,"layout":null,"metadata":null,"text":"Set up training Arguments.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_77":{"__typename":"Paragraph","id":"435b79f14100_77","name":"64b4","type":"P","href":null,"layout":null,"metadata":null,"text":"`output_dir` defines the directory where the training results will be written.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_78":{"__typename":"Paragraph","id":"435b79f14100_78","name":"7239","type":"P","href":null,"layout":null,"metadata":null,"text":"`per_device_train_batch_size` defines the batch size for each GPU. This could be increased or decreased, depending on the available GPU memory.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_79":{"__typename":"Paragraph","id":"435b79f14100_79","name":"6bd2","type":"P","href":null,"layout":null,"metadata":null,"text":"`gradient_accumulation_steps` defines the number of update steps to accumulate the gradients before performing a backward\u002Fupdate pass. This is used to increase the effective batch size without increasing the GPU memory usage.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_80":{"__typename":"Paragraph","id":"435b79f14100_80","name":"330c","type":"P","href":null,"layout":null,"metadata":null,"text":"`optim` defines the optimizer that will be used for training. The `paged_adamw_32bit` optimizer is a variant of `AdamW` that is designed to be more efficient on 32-bit GPUs.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":67,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":113,"end":119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_81":{"__typename":"Paragraph","id":"435b79f14100_81","name":"3904","type":"P","href":null,"layout":null,"metadata":null,"text":"`save_steps` defines the number of steps after which the model checkpoint will be saved.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_82":{"__typename":"Paragraph","id":"435b79f14100_82","name":"3fde","type":"P","href":null,"layout":null,"metadata":null,"text":"`fp16` defines whether to use 16-bit floating point precision during training. This can significantly reduce memory usage, but it may also reduce the accuracy of the model.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_83":{"__typename":"Paragraph","id":"435b79f14100_83","name":"d526","type":"P","href":null,"layout":null,"metadata":null,"text":"`logging_steps` defines the Number of update steps between two logs.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_84":{"__typename":"Paragraph","id":"435b79f14100_84","name":"5bc7","type":"P","href":null,"layout":null,"metadata":null,"text":"`learning_rate` defines the initial learning rate for the optimizer.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_85":{"__typename":"Paragraph","id":"435b79f14100_85","name":"0297","type":"P","href":null,"layout":null,"metadata":null,"text":"`max_grad_norm` defines the maximum norm of the gradients. This is used to prevent the gradients from becoming too large to prevent instability.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_86":{"__typename":"Paragraph","id":"435b79f14100_86","name":"8f47","type":"P","href":null,"layout":null,"metadata":null,"text":"`max_steps` defines the maximum number of steps to train for.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_87":{"__typename":"Paragraph","id":"435b79f14100_87","name":"6e96","type":"P","href":null,"layout":null,"metadata":null,"text":"`warmup_ratio` defines the ratio of the warm-up steps to the total number of steps. Warm-up steps gradually increase the learning rate, which helps to prevent the model from overfitting.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_88":{"__typename":"Paragraph","id":"435b79f14100_88","name":"eaba","type":"P","href":null,"layout":null,"metadata":null,"text":"`lr_scheduler_type` defines the type of learning rate scheduler that will be used. The `constant scheduler` keeps the learning rate constant for the entire training process. This could be increased or decreased, depending on the available GPU memory. Other values are `cosine_scheduler`, `linear_scheduler`, etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":88,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":269,"end":286,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":289,"end":306,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_89":{"__typename":"Paragraph","id":"435b79f14100_89","name":"157f","type":"P","href":null,"layout":null,"metadata":null,"text":"You may need to experiment with different values to determine the best values of the parameters for the specific language model you choose, the downstream task, and the available resources.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_90":{"__typename":"Paragraph","id":"435b79f14100_90","name":"11ca","type":"PRE","href":null,"layout":null,"metadata":null,"text":"training_arguments = TrainingArguments(\n    output_dir=outputs,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    save_steps=10,\n    logging_steps=10,\n    learning_rate=2e-4,\n    fp16=True,\n    max_grad_norm=0.3,\n    max_steps=150,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=constant\n)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_91":{"__typename":"Paragraph","id":"435b79f14100_91","name":"b2a7","type":"P","href":null,"layout":null,"metadata":null,"text":"Get the tokenizer specific to the pre-trained model and set the padding token to be the same as the end-of-sequence token.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_92":{"__typename":"Paragraph","id":"435b79f14100_92","name":"0906","type":"PRE","href":null,"layout":null,"metadata":null,"text":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_93":{"__typename":"Paragraph","id":"435b79f14100_93","name":"a1ae","type":"P","href":null,"layout":null,"metadata":null,"text":"In the following code snippet, we’ll initialize an object of the huggingface `SFTTrainer` class, and pass both the dataset and the data column that we want to train the model on, for our specific use case. We’ll also pass the peft configurations as inputs (to use the Lora configuration that we set earlier), tokenizer, maximum sequence length, model, and training arguments.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":77,"end":89,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_94":{"__typename":"Paragraph","id":"435b79f14100_94","name":"82f5","type":"P","href":null,"layout":null,"metadata":null,"text":"`SFTTrainer` is specifically optimized for Supervised Fine-tuning (SFT). It inherits from the Trainer class available in the Transformer library. We’ll import it from the `trl` library.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":173,"end":176,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_95":{"__typename":"Paragraph","id":"435b79f14100_95","name":"1e16","type":"PRE","href":null,"layout":null,"metadata":null,"text":"max_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=lora_model,\n    train_dataset=train_dict['train'],\n    peft_config=lora_config,\n    dataset_text_field=\"formatted_data\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_96":{"__typename":"Paragraph","id":"435b79f14100_96","name":"3c88","type":"P","href":null,"layout":null,"metadata":null,"text":"We will freeze the original weights of the model and keep the layer norm in float 32. Models are typically trained on 32-bit precision for higher accuracy. This step ensures more stable training. Following this, we will proceed with training the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_97":{"__typename":"Paragraph","id":"435b79f14100_97","name":"11da","type":"P","href":null,"layout":null,"metadata":null,"text":"This will cast the weights to higher precision floats in the layers at the time of computation resulting in a higher speed of fine-tuning.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_98":{"__typename":"Paragraph","id":"435b79f14100_98","name":"960e","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Loop through the named modules of the model\nfor name, module in trainer.model.named_modules():\n# Check if the name contains \"norm\"\n    if \"norm\" in name:\n # Convert the module to use torch.float32 data type\n        module = module.to(torch.float32)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_99":{"__typename":"Paragraph","id":"435b79f14100_99","name":"79a5","type":"H3","href":null,"layout":null,"metadata":null,"text":"Train the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_100":{"__typename":"Paragraph","id":"435b79f14100_100","name":"99ff","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Disabling cache usage in the model configuration\nlora_model.config.use_cache = False\n\ntrainer.train()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_101":{"__typename":"Paragraph","id":"435b79f14100_101","name":"9bc8","type":"H3","href":null,"layout":null,"metadata":null,"text":"Save the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_102":{"__typename":"Paragraph","id":"435b79f14100_102","name":"e0bc","type":"P","href":null,"layout":null,"metadata":null,"text":"This will only save the LoRA model adapter. For inference, we need to load both the saved adapter and the base Falcon 7B model. The `tuned_model` directory contains the adapter bin and config files that are generated at the end of the training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":133,"end":144,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_103":{"__typename":"Paragraph","id":"435b79f14100_103","name":"4031","type":"PRE","href":null,"layout":null,"metadata":null,"text":"lora_model.save_pretrained(\"tuned_model\u002F\")","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_104":{"__typename":"Paragraph","id":"435b79f14100_104","name":"e805","type":"H3","href":null,"layout":null,"metadata":null,"text":"Inference","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_105":{"__typename":"Paragraph","id":"435b79f14100_105","name":"3f4f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"peft_model = '.\u002Ftuned_model'\nconfig = PeftConfig.from_pretrained(peft_model)\n\npeft_base_model = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map='auto'\n    )\n\n# Load the Lora model\ntrained_model = PeftModel.from_pretrained(peft_base_model, peft_model)\ntrained_model_tokenizer = AutoTokenizer.from_pretrained(\n    config.base_model_name_or_path,\n    trust_remote_code=True\n)\n\ntrained_model_tokenizer.pad_token = trained_model_tokenizer.eos_toke","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_106":{"__typename":"Paragraph","id":"435b79f14100_106","name":"3130","type":"P","href":null,"layout":null,"metadata":null,"text":"Select a sample text from the test dataset for inference.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_107":{"__typename":"Paragraph","id":"435b79f14100_107","name":"a92a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"sample = test_df.iloc[0, :]\nsample_text = sample['text']","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_108":{"__typename":"Paragraph","id":"435b79f14100_108","name":"273b","type":"P","href":null,"layout":null,"metadata":null,"text":"Tokenize the sample text.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_109":{"__typename":"Paragraph","id":"435b79f14100_109","name":"1fde","type":"PRE","href":null,"layout":null,"metadata":null,"text":"batch = tokenizer(sample_text, return_tensors='pt').to(\"cuda\")","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_110":{"__typename":"Paragraph","id":"435b79f14100_110","name":"f774","type":"H4","href":null,"layout":null,"metadata":null,"text":"Create generation config for prediction.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_111":{"__typename":"Paragraph","id":"435b79f14100_111","name":"3928","type":"P","href":null,"layout":null,"metadata":null,"text":"We need to create a generation configuration for the inference.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_112":{"__typename":"Paragraph","id":"435b79f14100_112","name":"eed5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"gen_config = GenerationConfig(\n    max_new_tokens = 5,\n    attention_mask=batch.attention_mask,\n    pad_token_id = trained_model_tokenizer.pad_token_id,\n    eos_token_id = trained_model_tokenizer.eos_token_id,\n    repetition_penalty=2.0,\n    num_return_sequences=1\n )","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_113":{"__typename":"Paragraph","id":"435b79f14100_113","name":"169f","type":"P","href":null,"layout":null,"metadata":null,"text":"Pass the generation configuration into pytorch’s `inference_mode`. Pytorch inference mode is a better version of the torch.no_grad which disables computing gradients.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":50,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_114":{"__typename":"Paragraph","id":"435b79f14100_114","name":"b2f8","type":"PRE","href":null,"layout":null,"metadata":null,"text":"with torch.inference_mode():\n    result = trained_model.generate(\n        input_ids=batch.input_ids,\n        generation_config=gen_config,\n    )\n\nfinal_output = trained_model_tokenizer.decode(result[0], skip_special_tokens=True)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_115":{"__typename":"Paragraph","id":"435b79f14100_115","name":"14ad","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s is the generated output.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_116":{"__typename":"Paragraph","id":"435b79f14100_116","name":"f5c2","type":"PRE","href":null,"layout":null,"metadata":null,"text":"final_output","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XN1EXhUFkfFBo_OOuCeFlA.png":{"__typename":"ImageMetadata","id":"1*XN1EXhUFkfFBo_OOuCeFlA.png","originalHeight":44,"originalWidth":1597,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:435b79f14100_117":{"__typename":"Paragraph","id":"435b79f14100_117","name":"5665","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*XN1EXhUFkfFBo_OOuCeFlA.png"},"text":"Final output.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_118":{"__typename":"Paragraph","id":"435b79f14100_118","name":"19a6","type":"P","href":null,"layout":null,"metadata":null,"text":"I hope this article explained the QLoRA fine-tuning simply. You may want to learn more from the references below so that you can experiment further with configurations of the libraries used in this article.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_119":{"__typename":"Paragraph","id":"435b79f14100_119","name":"2d2c","type":"H3","href":null,"layout":null,"metadata":null,"text":"References.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_120":{"__typename":"Paragraph","id":"435b79f14100_120","name":"2f1d","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fv4.20.1\u002Fen\u002Fperf_train_gpu_one","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":70,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fv4.20.1\u002Fen\u002Fperf_train_gpu_one","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_121":{"__typename":"Paragraph","id":"435b79f14100_121","name":"fa67","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fgeneration_strategies","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":62,"href":"https:\u002F\u002Fcolab.research.google.com\u002Fcorgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fgeneration_strategies","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_122":{"__typename":"Paragraph","id":"435b79f14100_122","name":"c1dc","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fblog\u002F4bit-transformers-bitsandbytes","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":58,"href":"https:\u002F\u002Fhuggingface.co\u002Fblog\u002F4bit-transformers-bitsandbytes","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_123":{"__typename":"Paragraph","id":"435b79f14100_123","name":"85e4","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Fpeft\u002Fen\u002Fpackage_reference\u002Flora","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":58,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Fpeft\u002Fen\u002Fpackage_reference\u002Flora","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_124":{"__typename":"Paragraph","id":"435b79f14100_124","name":"bf1f","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Ftiiuae\u002Ffalcon-7b","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":39,"href":"https:\u002F\u002Fhuggingface.co\u002Ftiiuae\u002Ffalcon-7b","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_125":{"__typename":"Paragraph","id":"435b79f14100_125","name":"b350","type":"P","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fhuggingface.co\u002Fblog\u002Fpeft","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":32,"href":"https:\u002F\u002Fhuggingface.co\u002Fblog\u002Fpeft","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_126":{"__typename":"Paragraph","id":"435b79f14100_126","name":"0096","type":"P","href":null,"layout":null,"metadata":null,"text":"I look forward to your comments on this article.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_127":{"__typename":"Paragraph","id":"435b79f14100_127","name":"d0ac","type":"H3","href":null,"layout":null,"metadata":null,"text":"In Plain English 🚀","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_128":{"__typename":"Paragraph","id":"435b79f14100_128","name":"eba7","type":"P","href":null,"layout":null,"metadata":null,"text":"Thank you for being a part of the In Plain English community! Before you go:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":34,"end":50,"href":"https:\u002F\u002Fplainenglish.io","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":34,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_129":{"__typename":"Paragraph","id":"435b79f14100_129","name":"18c0","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Be sure to clap and follow the writer ️👏️️","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":11,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":20,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":41,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_130":{"__typename":"Paragraph","id":"435b79f14100_130","name":"5707","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Follow us: X | LinkedIn | YouTube | Discord | Newsletter","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":11,"end":12,"href":"https:\u002F\u002Ftwitter.com\u002FinPlainEngHQ","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":15,"end":23,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fcompany\u002Finplainenglish\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":26,"end":33,"href":"https:\u002F\u002Fwww.youtube.com\u002Fchannel\u002FUCtipWUghju290NWcn8jhyAw","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":36,"end":43,"href":"https:\u002F\u002Fdiscord.gg\u002Fin-plain-english-709094664682340443","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":46,"end":56,"href":"https:\u002F\u002Fnewsletter.plainenglish.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":11,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_131":{"__typename":"Paragraph","id":"435b79f14100_131","name":"8d35","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Visit our other platforms: Stackademic | CoFeed | Venture | Cubed","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":27,"end":38,"href":"https:\u002F\u002Fstackademic.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":41,"end":47,"href":"https:\u002F\u002Fcofeed.app\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":50,"end":57,"href":"https:\u002F\u002Fventuremagazine.net\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":60,"end":65,"href":"https:\u002F\u002Fblog.cubed.run","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":27,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:435b79f14100_132":{"__typename":"Paragraph","id":"435b79f14100_132","name":"9456","type":"ULI","href":null,"layout":null,"metadata":null,"text":"More content at PlainEnglish.io","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":16,"end":31,"href":"https:\u002F\u002Fplainenglish.io","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":16,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Tag:llm":{"__typename":"Tag","id":"llm","displayTitle":"Llm","normalizedTagSlug":"llm"},"Tag:fine-tuning-llm":{"__typename":"Tag","id":"fine-tuning-llm","displayTitle":"Fine Tuning Llm","normalizedTagSlug":"fine-tuning-llm"},"Tag:falcon-llm":{"__typename":"Tag","id":"falcon-llm","displayTitle":"Falcon Llm","normalizedTagSlug":"falcon-llm"},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:transformers":{"__typename":"Tag","id":"transformers","displayTitle":"Transformers","normalizedTagSlug":"transformers"},"Post:388dcfb1c7e9":{"__typename":"Post","id":"388dcfb1c7e9","collection":null,"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"5f61","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:435b79f14100_0"},{"__ref":"Paragraph:435b79f14100_1"},{"__ref":"Paragraph:435b79f14100_2"},{"__ref":"Paragraph:435b79f14100_3"},{"__ref":"Paragraph:435b79f14100_4"},{"__ref":"Paragraph:435b79f14100_5"},{"__ref":"Paragraph:435b79f14100_6"},{"__ref":"Paragraph:435b79f14100_7"},{"__ref":"Paragraph:435b79f14100_8"},{"__ref":"Paragraph:435b79f14100_9"},{"__ref":"Paragraph:435b79f14100_10"},{"__ref":"Paragraph:435b79f14100_11"},{"__ref":"Paragraph:435b79f14100_12"},{"__ref":"Paragraph:435b79f14100_13"},{"__ref":"Paragraph:435b79f14100_14"},{"__ref":"Paragraph:435b79f14100_15"},{"__ref":"Paragraph:435b79f14100_16"},{"__ref":"Paragraph:435b79f14100_17"},{"__ref":"Paragraph:435b79f14100_18"},{"__ref":"Paragraph:435b79f14100_19"},{"__ref":"Paragraph:435b79f14100_20"},{"__ref":"Paragraph:435b79f14100_21"},{"__ref":"Paragraph:435b79f14100_22"},{"__ref":"Paragraph:435b79f14100_23"},{"__ref":"Paragraph:435b79f14100_24"},{"__ref":"Paragraph:435b79f14100_25"},{"__ref":"Paragraph:435b79f14100_26"},{"__ref":"Paragraph:435b79f14100_27"},{"__ref":"Paragraph:435b79f14100_28"},{"__ref":"Paragraph:435b79f14100_29"},{"__ref":"Paragraph:435b79f14100_30"},{"__ref":"Paragraph:435b79f14100_31"},{"__ref":"Paragraph:435b79f14100_32"},{"__ref":"Paragraph:435b79f14100_33"},{"__ref":"Paragraph:435b79f14100_34"},{"__ref":"Paragraph:435b79f14100_35"},{"__ref":"Paragraph:435b79f14100_36"},{"__ref":"Paragraph:435b79f14100_37"},{"__ref":"Paragraph:435b79f14100_38"},{"__ref":"Paragraph:435b79f14100_39"},{"__ref":"Paragraph:435b79f14100_40"},{"__ref":"Paragraph:435b79f14100_41"},{"__ref":"Paragraph:435b79f14100_42"},{"__ref":"Paragraph:435b79f14100_43"},{"__ref":"Paragraph:435b79f14100_44"},{"__ref":"Paragraph:435b79f14100_45"},{"__ref":"Paragraph:435b79f14100_46"},{"__ref":"Paragraph:435b79f14100_47"},{"__ref":"Paragraph:435b79f14100_48"},{"__ref":"Paragraph:435b79f14100_49"},{"__ref":"Paragraph:435b79f14100_50"},{"__ref":"Paragraph:435b79f14100_51"},{"__ref":"Paragraph:435b79f14100_52"},{"__ref":"Paragraph:435b79f14100_53"},{"__ref":"Paragraph:435b79f14100_54"},{"__ref":"Paragraph:435b79f14100_55"},{"__ref":"Paragraph:435b79f14100_56"},{"__ref":"Paragraph:435b79f14100_57"},{"__ref":"Paragraph:435b79f14100_58"},{"__ref":"Paragraph:435b79f14100_59"},{"__ref":"Paragraph:435b79f14100_60"},{"__ref":"Paragraph:435b79f14100_61"},{"__ref":"Paragraph:435b79f14100_62"},{"__ref":"Paragraph:435b79f14100_63"},{"__ref":"Paragraph:435b79f14100_64"},{"__ref":"Paragraph:435b79f14100_65"},{"__ref":"Paragraph:435b79f14100_66"},{"__ref":"Paragraph:435b79f14100_67"},{"__ref":"Paragraph:435b79f14100_68"},{"__ref":"Paragraph:435b79f14100_69"},{"__ref":"Paragraph:435b79f14100_70"},{"__ref":"Paragraph:435b79f14100_71"},{"__ref":"Paragraph:435b79f14100_72"},{"__ref":"Paragraph:435b79f14100_73"},{"__ref":"Paragraph:435b79f14100_74"},{"__ref":"Paragraph:435b79f14100_75"},{"__ref":"Paragraph:435b79f14100_76"},{"__ref":"Paragraph:435b79f14100_77"},{"__ref":"Paragraph:435b79f14100_78"},{"__ref":"Paragraph:435b79f14100_79"},{"__ref":"Paragraph:435b79f14100_80"},{"__ref":"Paragraph:435b79f14100_81"},{"__ref":"Paragraph:435b79f14100_82"},{"__ref":"Paragraph:435b79f14100_83"},{"__ref":"Paragraph:435b79f14100_84"},{"__ref":"Paragraph:435b79f14100_85"},{"__ref":"Paragraph:435b79f14100_86"},{"__ref":"Paragraph:435b79f14100_87"},{"__ref":"Paragraph:435b79f14100_88"},{"__ref":"Paragraph:435b79f14100_89"},{"__ref":"Paragraph:435b79f14100_90"},{"__ref":"Paragraph:435b79f14100_91"},{"__ref":"Paragraph:435b79f14100_92"},{"__ref":"Paragraph:435b79f14100_93"},{"__ref":"Paragraph:435b79f14100_94"},{"__ref":"Paragraph:435b79f14100_95"},{"__ref":"Paragraph:435b79f14100_96"},{"__ref":"Paragraph:435b79f14100_97"},{"__ref":"Paragraph:435b79f14100_98"},{"__ref":"Paragraph:435b79f14100_99"},{"__ref":"Paragraph:435b79f14100_100"},{"__ref":"Paragraph:435b79f14100_101"},{"__ref":"Paragraph:435b79f14100_102"},{"__ref":"Paragraph:435b79f14100_103"},{"__ref":"Paragraph:435b79f14100_104"},{"__ref":"Paragraph:435b79f14100_105"},{"__ref":"Paragraph:435b79f14100_106"},{"__ref":"Paragraph:435b79f14100_107"},{"__ref":"Paragraph:435b79f14100_108"},{"__ref":"Paragraph:435b79f14100_109"},{"__ref":"Paragraph:435b79f14100_110"},{"__ref":"Paragraph:435b79f14100_111"},{"__ref":"Paragraph:435b79f14100_112"},{"__ref":"Paragraph:435b79f14100_113"},{"__ref":"Paragraph:435b79f14100_114"},{"__ref":"Paragraph:435b79f14100_115"},{"__ref":"Paragraph:435b79f14100_116"},{"__ref":"Paragraph:435b79f14100_117"},{"__ref":"Paragraph:435b79f14100_118"},{"__ref":"Paragraph:435b79f14100_119"},{"__ref":"Paragraph:435b79f14100_120"},{"__ref":"Paragraph:435b79f14100_121"},{"__ref":"Paragraph:435b79f14100_122"},{"__ref":"Paragraph:435b79f14100_123"},{"__ref":"Paragraph:435b79f14100_124"},{"__ref":"Paragraph:435b79f14100_125"},{"__ref":"Paragraph:435b79f14100_126"},{"__ref":"Paragraph:435b79f14100_127"},{"__ref":"Paragraph:435b79f14100_128"},{"__ref":"Paragraph:435b79f14100_129"},{"__ref":"Paragraph:435b79f14100_130"},{"__ref":"Paragraph:435b79f14100_131"},{"__ref":"Paragraph:435b79f14100_132"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:a22196dac4ca"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@bloomfountaincoder\u002Ffine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9","primaryTopic":null,"topics":[{"__typename":"Topic","slug":"machine-learning"}],"isPublished":true,"latestPublishedVersion":"435b79f14100","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":1},"createdAt":1701403474785,"firstPublishedAt":1710051830330,"latestPublishedAt":1710306797175,"clapCount":11,"allowResponses":true,"isLimitedState":false,"title":"Fine-tune Falcon 7b LLM on Custom Dataset for Sentiment Analysis Using QLoRA.","isSeries":false,"sequence":null,"uniqueSlug":"fine-tune-falcon-7b-llm-on-custom-dataset-for-sentiment-analysis-using-qlora-388dcfb1c7e9","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":9.38805031446541,"previewContent":{"__typename":"PreviewContent","subtitle":"Fine-tune Falcon 7B LLM."},"previewImage":{"__ref":"ImageMetadata:0*9PSSJMGe60zzIkKE"},"isShortform":false,"seoTitle":"","updatedAt":1710306800085,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:llm"},{"__ref":"Tag:fine-tuning-llm"},{"__ref":"Tag:falcon-llm"},{"__ref":"Tag:machine-learning"},{"__ref":"Tag:transformers"}],"isNewsletter":false,"statusForCollection":null,"pendingCollection":null,"detectedLanguage":"en","wordCount":2267,"layerCake":0}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT"}}</script><script src="./Tutorial_files/manifest.0a2eb996.js"></script><script src="./Tutorial_files/4328.87c721ec.js"></script><script src="./Tutorial_files/main.9aba1416.js"></script><script src="./Tutorial_files/instrumentation.4ddbf12e.chunk.js"></script>
<script src="./Tutorial_files/reporting.2021fe63.chunk.js"></script>
<script src="./Tutorial_files/4398.db4d4378.chunk.js"></script>
<script src="./Tutorial_files/7883.0e445e04.chunk.js"></script>
<script src="./Tutorial_files/9281.e9be8bce.chunk.js"></script>
<script src="./Tutorial_files/7111.b294e9da.chunk.js"></script>
<script src="./Tutorial_files/6481.9389fa5e.chunk.js"></script>
<script src="./Tutorial_files/8695.17d1af21.chunk.js"></script>
<script src="./Tutorial_files/3418.eb013b5a.chunk.js"></script>
<script src="./Tutorial_files/5971.2133e397.chunk.js"></script>
<script src="./Tutorial_files/5514.32e692f6.chunk.js"></script>
<script src="./Tutorial_files/5203.e7f058c5.chunk.js"></script>
<script src="./Tutorial_files/7098.93054372.chunk.js"></script>
<script src="./Tutorial_files/8051.5fec5149.chunk.js"></script>
<script src="./Tutorial_files/8558.54b7dad5.chunk.js"></script>
<script src="./Tutorial_files/1711.7605eb3e.chunk.js"></script>
<script src="./Tutorial_files/8597.a4328caa.chunk.js"></script>
<script src="./Tutorial_files/9174.485dada8.chunk.js"></script>
<script src="./Tutorial_files/8883.2f95bbf4.chunk.js"></script>
<script src="./Tutorial_files/705.88ee64f1.chunk.js"></script>
<script src="./Tutorial_files/5781.9f2ea9e7.chunk.js"></script>
<script src="./Tutorial_files/8580.feeb2549.chunk.js"></script>
<script src="./Tutorial_files/6046.f9be485b.chunk.js"></script>
<script src="./Tutorial_files/1525.9d7ce475.chunk.js"></script>
<script src="./Tutorial_files/500.9a872e1f.chunk.js"></script>
<script src="./Tutorial_files/9408.8ab2f0e9.chunk.js"></script>
<script src="./Tutorial_files/6605.84e81b15.chunk.js"></script>
<script src="./Tutorial_files/2790.ca0e202b.chunk.js"></script>
<script src="./Tutorial_files/7421.3f94f5ea.chunk.js"></script>
<script src="./Tutorial_files/51.d8fb2684.chunk.js"></script>
<script src="./Tutorial_files/PostPage.MainContent.44471406.chunk.js"></script><script>window.main();</script><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js/vef91dfe02fce4ee0ad053f6de4f175db1715022073587" integrity="sha512-sDIX0kl85v1Cl5tu4WGLZCpH/dV9OHbA4YlKCuCiMmOQIk4buzoYDZSFj+TvC71mOBLh8CDC/REgE0GX0xcbjA==" data-cf-beacon="{&quot;rayId&quot;:&quot;88dde9f2d96a383e&quot;,&quot;version&quot;:&quot;2024.4.1&quot;,&quot;token&quot;:&quot;0b5f665943484354a59c39c6833f7078&quot;}" crossorigin="anonymous"></script>
<script src="./Tutorial_files/client" async=""></script><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" width="256" height="60" role="presentation" name="a-q9wnvl46id7k" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox" src="./Tutorial_files/anchor.html"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;" src="./Tutorial_files/saved_resource.html"></iframe></div></body></html>