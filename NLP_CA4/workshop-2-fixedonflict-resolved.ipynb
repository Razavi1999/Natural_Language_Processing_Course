{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"install and import libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q trl xformers wandb datasets einops gradio sentencepiece bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T13:12:35.534300Z","iopub.execute_input":"2024-05-15T13:12:35.534582Z","iopub.status.idle":"2024-05-15T13:16:55.931553Z","shell.execute_reply.started":"2024-05-15T13:12:35.534558Z","shell.execute_reply":"2024-05-15T13:16:55.930493Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall torch -y\n!pip install torch==2.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb, platform, gradio, warnings\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom huggingface_hub import notebook_login\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:25:31.381864Z","iopub.execute_input":"2024-05-15T13:25:31.382270Z","iopub.status.idle":"2024-05-15T13:25:31.389152Z","shell.execute_reply.started":"2024-05-15T13:25:31.382237Z","shell.execute_reply":"2024-05-15T13:25:31.387951Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"define the model and the dataset path","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3-8B\"\n\ndataset_name = \"sinarashidi/sentiment-analysis\"","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:26:29.644708Z","iopub.execute_input":"2024-05-15T13:26:29.645398Z","iopub.status.idle":"2024-05-15T13:26:29.649573Z","shell.execute_reply.started":"2024-05-15T13:26:29.645367Z","shell.execute_reply":"2024-05-15T13:26:29.648595Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"login to huggingface to download llama 3","metadata":{}},{"cell_type":"code","source":"notebook_login()\n# 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","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:26:57.622326Z","iopub.execute_input":"2024-05-15T13:26:57.623057Z","iopub.status.idle":"2024-05-15T13:26:57.645830Z","shell.execute_reply.started":"2024-05-15T13:26:57.623025Z","shell.execute_reply":"2024-05-15T13:26:57.644945Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b572c593de244570a9f26f8448dd3d30"}},"metadata":{}}]},{"cell_type":"markdown","source":"define quatization parameters","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:27:37.239155Z","iopub.execute_input":"2024-05-15T13:27:37.239519Z","iopub.status.idle":"2024-05-15T13:27:37.245568Z","shell.execute_reply.started":"2024-05-15T13:27:37.239490Z","shell.execute_reply":"2024-05-15T13:27:37.244482Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"download quantized llama 3's weights","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config= bnb_config,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:28:04.859000Z","iopub.execute_input":"2024-05-15T13:28:04.859332Z","iopub.status.idle":"2024-05-15T13:30:02.368496Z","shell.execute_reply.started":"2024-05-15T13:28:04.859308Z","shell.execute_reply":"2024-05-15T13:30:02.367558Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e15c796e8294e64823f2c5d9d8240ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f406bb77b3b14cafae0b8bc00beeec04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d682735c1f3e40b0858c32fd4e6d2001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef5626257c74dcab64555ddbe507f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f779607364849fb8a1a193c4ae34dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e97abdb69bd4633b3d95749b50deb19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3813b0b9e77e4361b7615d1b05f652fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d76b04d6f3549ba9767b88dd1eb5d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f52990acbb436d8da7157adbd4d6c4"}},"metadata":{}}]},{"cell_type":"markdown","source":"download and prepare model's tokenizer","metadata":{}},{"cell_type":"code","source":"# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:31:20.099986Z","iopub.execute_input":"2024-05-15T13:31:20.100880Z","iopub.status.idle":"2024-05-15T13:31:21.578110Z","shell.execute_reply.started":"2024-05-15T13:31:20.100845Z","shell.execute_reply":"2024-05-15T13:31:21.577110Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdccd70ec8ca4dffbaaf99504a7f70f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e7bea9de37748abbae3f2e6b3f96176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d440d6cd5a0452197f48bbbd6a70def"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"prepare the model for k-bit training","metadata":{}},{"cell_type":"code","source":"# model.gradient_checkpointing_enable()\n\nmodel = prepare_model_for_kbit_training(model)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:31:24.020741Z","iopub.execute_input":"2024-05-15T13:31:24.021868Z","iopub.status.idle":"2024-05-15T13:31:24.076691Z","shell.execute_reply.started":"2024-05-15T13:31:24.021825Z","shell.execute_reply":"2024-05-15T13:31:24.075840Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"download the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:31:51.512955Z","iopub.execute_input":"2024-05-15T13:31:51.513787Z","iopub.status.idle":"2024-05-15T13:31:55.810963Z","shell.execute_reply.started":"2024-05-15T13:31:51.513740Z","shell.execute_reply":"2024-05-15T13:31:55.810249Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/451 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39b2edc5472f457b832edfcfe62470d9"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 19.7M/19.7M [00:01<00:00, 14.5MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/128432 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429e21c0d0bd4b08b1392b08c6e5bd4d"}},"metadata":{}}]},{"cell_type":"markdown","source":"testing the model before training","metadata":{}},{"cell_type":"code","source":"sample_input = dataset['train'][0]['text']\ntokenized_sample_input = tokenizer.encode_plus(sample_input, return_tensors='pt')\ntokenized_sample_input.to(device)\nprint(sample_input)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:39:13.103367Z","iopub.execute_input":"2024-05-15T13:39:13.103733Z","iopub.status.idle":"2024-05-15T13:39:13.110724Z","shell.execute_reply.started":"2024-05-15T13:39:13.103704Z","shell.execute_reply":"2024-05-15T13:39:13.109734Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<s>[INST] Sentiment Detection: تازه توی پیشنهاد ویژه خریدم  کار راه اندازه [/INST] مثبت </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_input = dataset['train'][0]['text'].split('[/INST]')[0]\ntokenized_sample_input = tokenizer.encode_plus(sample_input, return_tensors='pt')\ntokenized_sample_input.to(device)\nprint(sample_input)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:39:16.482299Z","iopub.execute_input":"2024-05-15T13:39:16.483102Z","iopub.status.idle":"2024-05-15T13:39:16.489595Z","shell.execute_reply.started":"2024-05-15T13:39:16.483071Z","shell.execute_reply":"2024-05-15T13:39:16.488644Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"<s>[INST] Sentiment Detection: تازه توی پیشنهاد ویژه خریدم  کار راه اندازه \n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  output = model.generate(**tokenized_sample_input, pad_token_id=tokenizer.pad_token_id, max_new_tokens = 50)\n  print(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:37:29.549849Z","iopub.execute_input":"2024-05-15T13:37:29.550710Z","iopub.status.idle":"2024-05-15T13:37:35.226315Z","shell.execute_reply.started":"2024-05-15T13:37:29.550677Z","shell.execute_reply":"2024-05-15T13:37:35.225392Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"<|begin_of_text|><s>[INST] Sentiment Detection: تازه توی پیشنهاد ویژه خریدم  کار راه اندازه  </s>\n<s>[INST] Sentiment Detection:  فروش  </s>\n<s>[INST] Sentiment Detection:  کار راه اندازه  </s>\n<s>[INST] Sentiment Detection:  فروش  </s>\n<s>[INST\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_input_2 = dataset['train'][100]['text'].split('[/INST]')[0] + '[/INST]'\ntokenized_sample_input_2 = tokenizer.encode_plus(sample_input_2, return_tensors='pt')\ntokenized_sample_input_2.to(device)\nprint(sample_input_2)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:39:52.117507Z","iopub.execute_input":"2024-05-15T13:39:52.117927Z","iopub.status.idle":"2024-05-15T13:39:52.125034Z","shell.execute_reply.started":"2024-05-15T13:39:52.117878Z","shell.execute_reply":"2024-05-15T13:39:52.124111Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<s>[INST] Sentiment Detection: لطفا با توجه به نیاز دانشجویان به  لپ تاپ های 14 اینچ ، لپ تاپ زیر رو هم بیارید  ASUSPRO P5430UA  با تشکر [/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  output = model.generate(**tokenized_sample_input_2, pad_token_id=tokenizer.pad_token_id, max_new_tokens = 50)\n  print(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:40:21.591339Z","iopub.execute_input":"2024-05-15T13:40:21.592112Z","iopub.status.idle":"2024-05-15T13:40:27.175478Z","shell.execute_reply.started":"2024-05-15T13:40:21.592077Z","shell.execute_reply":"2024-05-15T13:40:27.174566Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<|begin_of_text|><s>[INST] Sentiment Detection: لطفا با توجه به نیاز دانشجویان به  لپ تاپ های 14 اینچ ، لپ تاپ زیر رو هم بیارید  ASUSPRO P5430UA  با تشکر [/INST] </s>\n<s>[INST] Sentiment Detection: لطفا با توجه به نیاز دانشجویان به  لپ تاپ های 14 اینچ ، لپ تاپ زیر رو هم بیارید  ASUSPRO P5430UA  با\n","output_type":"stream"}]},{"cell_type":"markdown","source":"LoRA config","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig(\n    lora_alpha= 8,\n    lora_dropout= 0.1,\n    r= 16,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:41:18.457629Z","iopub.execute_input":"2024-05-15T13:41:18.458351Z","iopub.status.idle":"2024-05-15T13:41:19.156149Z","shell.execute_reply.started":"2024-05-15T13:41:18.458318Z","shell.execute_reply":"2024-05-15T13:41:19.155131Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"pre-process and tokenize the dataset","metadata":{}},{"cell_type":"code","source":"mapped_dataset = dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\nmapped_dataset = mapped_dataset.filter(lambda samples: len(samples['input_ids']) < 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:41:45.857752Z","iopub.execute_input":"2024-05-15T13:41:45.858889Z","iopub.status.idle":"2024-05-15T13:42:15.549890Z","shell.execute_reply.started":"2024-05-15T13:41:45.858855Z","shell.execute_reply":"2024-05-15T13:42:15.548943Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/128432 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233ec7a0d24b4fb995846236517a3336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/128432 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b68fea5e25490eb049693825961f1a"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_set = mapped_dataset['train'][40000:]\ntest_set = mapped_dataset['train'][0:40000]\n\ntrain_set = Dataset.from_dict(train_set)\ntest_set = Dataset.from_dict(test_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:42:15.551960Z","iopub.execute_input":"2024-05-15T13:42:15.552392Z","iopub.status.idle":"2024-05-15T13:42:34.927118Z","shell.execute_reply.started":"2024-05-15T13:42:15.552352Z","shell.execute_reply":"2024-05-15T13:42:34.926300Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"training arguments and train","metadata":{}},{"cell_type":"code","source":"'''\ntraining_arguments = TrainingArguments(\n    # output_dir= \"./results\",\n    # num_train_epochs= 1,\n    # per_device_train_batch_size= 8,\n    # gradient_accumulation_steps= 2,\n    # optim = \"paged_adamw_8bit\",\n    save_steps= 1000,\n    logging_steps= 30,\n    learning_rate= 2e-4,\n    weight_decay= 0.001,\n    fp16= False,\n    bf16= False,\n    max_grad_norm= 0.3,\n    max_steps= -1,\n    warmup_ratio= 0.3,\n    group_by_length= True,\n    lr_scheduler_type= \"linear\",\n    # report_to=\"wandb\",\n)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_set,\n    # train_dataset=mapped_dataset,\n    args=transformers.TrainingArguments(\n        # num_train_epochs= 1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        logging_steps= 30,\n        warmup_steps=2,\n        max_steps=1000,\n        learning_rate=2e-4,\n        weight_decay= 0.001,\n        fp16= False,\n        bf16= False,\n        max_grad_norm= 0.3,\n        group_by_length= True,\n        optim=\"paged_adamw_8bit\",\n        output_dir= \"/content/drive/MyDrive/workshop\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\nmodel.train()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:42:54.817754Z","iopub.execute_input":"2024-05-15T13:42:54.818591Z","iopub.status.idle":"2024-05-15T14:05:59.667179Z","shell.execute_reply.started":"2024-05-15T13:42:54.818557Z","shell.execute_reply":"2024-05-15T14:05:59.665556Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240515_134935-cvpmu52p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/danial-organization/huggingface/runs/cvpmu52p' target=\"_blank\">/content/drive/MyDrive/workshop</a></strong> to <a href='https://wandb.ai/danial-organization/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/danial-organization/huggingface' target=\"_blank\">https://wandb.ai/danial-organization/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/danial-organization/huggingface/runs/cvpmu52p' target=\"_blank\">https://wandb.ai/danial-organization/huggingface/runs/cvpmu52p</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='41' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  41/1000 13:53 < 5:41:34, 0.05 it/s, Epoch 0.01/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>30</td>\n      <td>2.978500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m    159\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_stats\u001b[39m(device: Union[Device, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dictionary of CUDA memory allocator statistics for a\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    given device.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m        meaningful, and are always reported as zero.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  output = model.generate(**tokenized_sample_input, pad_token_id=tokenizer.pad_token_id, max_new_tokens = 50)\n  print(tokenizer.decode(output[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_input_2 = dataset['train'][-1000]['text'].split('[/INST]')[0] + '[/INST]'\ntokenized_sample_input_2 = tokenizer.encode_plus(sample_input_2, return_tensors='pt')\ntokenized_sample_input_2.to(device)\nprint(sample_input_2)\n\nmodel.eval()\nwith torch.no_grad():\n  output = model.generate(**tokenized_sample_input_2, pad_token_id=tokenizer.pad_token_id, max_new_tokens = 50)\n  print(tokenizer.decode(output[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}