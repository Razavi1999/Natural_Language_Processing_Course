{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "pwEOrlJVuO_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Version of Regular Expression"
      ],
      "metadata": {
        "id": "F-wuX_wbT4of"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP4DR4vZ32zy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "  pattern = r'\\b\\w+\\b'\n",
        "  tokens = re.findall(pattern , text)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = '- Just received my M.Sc. diploma today, on 2024/02/10! Exicted to embark on this new journey of knowledge and discovery.#MScGraduate #EducationMatters.'\n",
        "\n",
        "custom_tokenizer(sample)\n",
        "\n",
        "# بر حسب کلمه توکن ساخته است."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqAH7LvN5Ic_",
        "outputId": "2180e47b-1266-40dc-caee-2d055d291193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'received',\n",
              " 'my',\n",
              " 'M',\n",
              " 'Sc',\n",
              " 'diploma',\n",
              " 'today',\n",
              " 'on',\n",
              " '2024',\n",
              " '02',\n",
              " '10',\n",
              " 'Exicted',\n",
              " 'to',\n",
              " 'embark',\n",
              " 'on',\n",
              " 'this',\n",
              " 'new',\n",
              " 'journey',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'and',\n",
              " 'discovery',\n",
              " 'MScGraduate',\n",
              " 'EducationMatters']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified Version of Regular Expression"
      ],
      "metadata": {
        "id": "kfSO-pVNTzNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def custom_tokenizer_modified(text):\n",
        "    pattern = r\"\\b[\\w'-\\/.]+\\b\"  # Updated pattern to include backslashes and dashes apostrophes and hyphens\n",
        "    tokens = re.findall(pattern, text, re.IGNORECASE)  # Added re.IGNORECASE flag for case insensitivity\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "kZc85RINK3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = '- Just received my M.Sc. diploma today, on 2024/02/10! \\\n",
        "Exicted to embark on this new journey of knowledge and discovery.#MScGraduate #EducationMatters.\\\n",
        " don\\'t cat\\'t Ahmad-Norrollahi'\n",
        "custom_tokenizer_modified(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTXI9Y1LTGMG",
        "outputId": "cdf5c24e-0f31-463d-d959-0afcc2a13b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'received',\n",
              " 'my',\n",
              " 'M.Sc',\n",
              " 'diploma',\n",
              " 'today',\n",
              " 'on',\n",
              " '2024/02/10',\n",
              " 'Exicted',\n",
              " 'to',\n",
              " 'embark',\n",
              " 'on',\n",
              " 'this',\n",
              " 'new',\n",
              " 'journey',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'and',\n",
              " 'discovery',\n",
              " 'MScGraduate',\n",
              " 'EducationMatters',\n",
              " \"don't\",\n",
              " \"cat't\",\n",
              " 'Ahmad-Norrollahi']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "Y1EdN8c4uSuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wordpiece tokenizer**"
      ],
      "metadata": {
        "id": "GH5yfLVOCi2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path0 = \"/content/drive/MyDrive/NLP_CA1/All_Around_the_Moon.txt\"\n",
        "data_file0 = open(data_path0 , 'r')\n",
        "\n",
        "# n = 1230\n",
        "# bpe_pairs = byte_pair_encoding(data_file0, n)\n",
        "# bpe_pairs\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, processors\n",
        "\n",
        "# Define the WordPiece model\n",
        "model = models.WordPiece()\n",
        "\n",
        "# Initialize the tokenizer with the WordPiece model\n",
        "tokenizer = Tokenizer(model)\n",
        "\n",
        "# Load your corpus into a list of strings\n",
        "# corpus = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
        "corpus = data_file0\n",
        "\n",
        "# Train the tokenizer on the corpus\n",
        "trainer = trainers.WordPieceTrainer()\n",
        "tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "print(\"Vocabulary size : \", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5RCJLD2NYsW",
        "outputId": "2b9a5984-d629-4576-c667-a0dc5e3542b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size :  30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE Tokenizer**"
      ],
      "metadata": {
        "id": "2EhYk-LfmD60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path0 = \"/content/drive/MyDrive/NLP_CA1/All_Around_the_Moon.txt\"\n",
        "data_file0 = open(data_path0 , 'r')\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers\n",
        "\n",
        "# Define the Byte Pair Encoding (BPE) model\n",
        "model = models.BPE()\n",
        "\n",
        "# Initialize the tokenizer with the BPE model\n",
        "tokenizer = Tokenizer(model)\n",
        "\n",
        "# Load your corpus into a list of strings\n",
        "# corpus = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
        "\n",
        "corpus = data_file0\n",
        "\n",
        "# Train the tokenizer on the corpus\n",
        "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "# trainer = trainers.BpeTrainer()\n",
        "\n",
        "tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "print(\"Vocabulary size : \" , vocab_size)\n",
        "\n",
        "# for i in range(0 , 20):\n",
        "#   print(tokenizer.get_vocab()[i] , end = ' ')\n"
      ],
      "metadata": {
        "id": "IrAxwlSy8JYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3"
      ],
      "metadata": {
        "id": "PH2b61Y0uVUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zXt6wPwuXDh",
        "outputId": "c2a15237-eb45-417a-8e49-d5c60f9cc4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/NLP_CA1/Tarzan.txt\"\n",
        "\n",
        "with open(data_path, 'r') as data_file:\n",
        "    text_data = data_file.read()"
      ],
      "metadata": {
        "id": "6ufTlx9h3WYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "aW6C9USO3c90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# Remove special characters using regex\n",
        "text_data = re.sub(r'[^a-zA-Z0-9\\s]', '', text_data)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_data)"
      ],
      "metadata": {
        "id": "i3JDHeJS3Mio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementation of Bigram model"
      ],
      "metadata": {
        "id": "J8Ebu3e6kGLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP_CA1/Tarzan.txt\"\n",
        "\n",
        "with open(data_path, 'r') as data_file:\n",
        "    text_data = data_file.read()\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# Remove special characters using regex\n",
        "text_data = re.sub(r'[^a-zA-Z0-9\\s]', '', text_data)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_data)\n",
        "\n",
        "# Remove Stop Words\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if not word in stops]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Create a dictionary to store the co-occurrence matrix for bigrams\n",
        "co_occurrence_matrix = {} # defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "all_words = []\n",
        "\n",
        "# Construct the co-occurrence matrix for bigrams\n",
        "for i in range(len(tokens) - 1):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "\n",
        "    key = (word1 , word2)\n",
        "    key2 = (word2 , word1)\n",
        "\n",
        "    if key not in co_occurrence_matrix.keys():\n",
        "      co_occurrence_matrix[key] = 1\n",
        "      co_occurrence_matrix[key2] = 1\n",
        "      all_words.append(word1)\n",
        "      all_words.append(word2)\n",
        "\n",
        "    else :\n",
        "      co_occurrence_matrix[key] += 1\n",
        "      co_occurrence_matrix[key2] += 1\n",
        "\n",
        "\n",
        "# for each in co_occurrence_matrix:\n",
        "#   if co_occurrence_matrix[each] == 0 :\n",
        "#     print(f'{each} : {co_occurrence_matrix[each]}')\n",
        "\n",
        "#   if 'got' in each or 'love' in each:\n",
        "#     print(f'{each} : {co_occurrence_matrix[each]}')\n",
        "\n",
        "print(f'all_words count  : {len(all_words)}')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "\n",
        "# Evaluating probability of occurence.\n",
        "count = {}\n",
        "probability = {}\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "my_counter = Counter()\n",
        "count = {}\n",
        "\n",
        "for word in tokens:\n",
        "  if word in count.keys():\n",
        "    count[word] += 1\n",
        "\n",
        "  else:\n",
        "    count[word] = 1\n",
        "\n",
        "\n",
        "# THIS SECTION SHOULD BE RUN\n",
        "\n",
        "for i in range(len(tokens) - 1):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "\n",
        "    key = (word1 , word2)\n",
        "\n",
        "    probability[key] = co_occurrence_matrix[key] / count[word1]\n",
        "\n",
        "    # if probability[key] > 1:\n",
        "    #     print(f'{key} : {probability[key]}')\n",
        "\n",
        "    # if 'got' in key or 'love' in key:\n",
        "    #    print(f'{key} : {probability[key]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apUJRfM6kJt9",
        "outputId": "6def10ad-9fe7-46ee-dd58-2331d09e11be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_words count  : 59618\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text1 = ['Knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
        "\n",
        "last_word = generated_text1[-1]\n",
        "maximum_value = 0\n",
        "maximum_key = 0\n",
        "\n",
        "for i in range(10):\n",
        "  maximum_value = 0\n",
        "  maximum_key = 0\n",
        "  last_word = generated_text1[-1]\n",
        "\n",
        "  # print(f'last_word : {last_word}')\n",
        "  # print('------------------------------------')\n",
        "\n",
        "  for sequence in probability:\n",
        "    if last_word in sequence:\n",
        "      candidate_key = (sequence[1] if (sequence[0] == last_word) else sequence[0]  )\n",
        "\n",
        "      # print(f'{sequence} : {probability[sequence]}')\n",
        "\n",
        "      if candidate_key in generated_text1[-2:]:\n",
        "        continue\n",
        "\n",
        "\n",
        "      candidate_value = probability[sequence]\n",
        "\n",
        "      if candidate_value > maximum_value:\n",
        "        maximum_value = candidate_value\n",
        "        maximum_key = candidate_key\n",
        "\n",
        "  # print(f'maximum_key : {maximum_key}')\n",
        "  generated_text1.append(maximum_key)\n",
        "\n",
        "\n",
        "print(\" \".join(generated_text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z75HvpysAEnR",
        "outputId": "e7d5311b-9229-4286-b3e9-55a80b7da1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowing well the windings of the trail he come assembl compani fag princess brynilda wife affianc malud assert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementation of 3-gram model"
      ],
      "metadata": {
        "id": "iDrCzNGet4ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP_CA1/Tarzan.txt\"\n",
        "\n",
        "with open(data_path, 'r') as data_file:\n",
        "    text_data = data_file.read()\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# Remove special characters using regex\n",
        "text_data = re.sub(r'[^a-zA-Z0-9\\s]', '', text_data)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_data)\n",
        "\n",
        "# Remove Stop Words\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if not word in stops]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Create a dictionary to store the co-occurrence matrix for bigrams\n",
        "co_occurrence_matrix_3 = {} # defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "all_pairs = []\n",
        "all_terneries = []\n",
        "\n",
        "# Construct the co-occurrence matrix for bigrams\n",
        "for i in range(len(tokens) - 2):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "    word3 = tokens[i + 2]\n",
        "\n",
        "    key_bin  =  (word1 , word2)\n",
        "    key_ternary = (word1 , word2 , word3)\n",
        "\n",
        "    if key_bin not in co_occurrence_matrix_3.keys():\n",
        "      co_occurrence_matrix_3[key_bin] = 1\n",
        "      all_pairs.append(key_bin)\n",
        "\n",
        "    if key_ternary not in co_occurrence_matrix_3.keys():\n",
        "      co_occurrence_matrix_3[key_ternary] = 1\n",
        "      all_terneries.append(key_ternary)\n",
        "\n",
        "    else :\n",
        "      co_occurrence_matrix_3[key_bin] += 1\n",
        "      co_occurrence_matrix_3[key_ternary] += 1\n",
        "\n",
        "\n",
        "# for each in co_occurrence_matrix_3:\n",
        "#   if co_occurrence_matrix_3[each] == 0 :\n",
        "#     print(f'{each} : {co_occurrence_matrix_3[each]}')\n",
        "\n",
        "#   if 'got' in each or 'love' in each:\n",
        "#     print(f'{each} : {co_occurrence_matrix_3[each]}')\n",
        "\n",
        "print(f'all_pairs count  : {len(all_pairs)}')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "print(f'all_terneris count  : {len(all_terneries)}')\n",
        "\n",
        "# Evaluating probability of occurence.\n",
        "probability_3 = {}\n",
        "\n",
        "\n",
        "# THIS SECTION SHOULD BE RUN\n",
        "\n",
        "for i in range(len(tokens) - 2):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "    word3 = tokens[i + 2]\n",
        "\n",
        "    key_bin = (word1 , word2)\n",
        "    key_ternary = (word1 , word2 , word3)\n",
        "\n",
        "\n",
        "    probability_3[(word3 , key_bin)] = co_occurrence_matrix_3[key_ternary] / co_occurrence_matrix_3[key_bin]\n",
        "\n",
        "    if probability_3[(word3 , key_bin)] < 1:\n",
        "        print(f'{ (word3 , key_bin) } : {probability_3[(word3 , key_bin)]}')\n",
        "\n",
        "    if 'got' in key_bin : # or 'love' in key_ternary:\n",
        "       print(f'{(word3 , key_bin)} : {probability_3[(word3 , key_bin)]}')\n"
      ],
      "metadata": {
        "id": "inmutr45t8bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test 3-gram**"
      ],
      "metadata": {
        "id": "jItWlQ68t929"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text1 = ['Knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
        "\n",
        "last_words = (generated_text1[-2] , generated_text1[-1])\n",
        "maximum_value = 0\n",
        "maximum_key = 0\n",
        "\n",
        "for i in range(10):\n",
        "  maximum_value = 0\n",
        "  maximum_key = 0\n",
        "  last_words = (generated_text1[-2] , generated_text1[-1])\n",
        "\n",
        "  # print(f'last_words : {last_words}')\n",
        "  # print('------------------------------------')\n",
        "\n",
        "  for sequence in probability_3:\n",
        "    if last_words[1] in sequence[1]:\n",
        "      candidate_key = sequence[0]\n",
        "\n",
        "      # print(f'{sequence} : {probability_3[sequence]}')\n",
        "\n",
        "      if candidate_key in generated_text1[-3:]:\n",
        "        continue\n",
        "\n",
        "      candidate_value = probability_3[sequence]\n",
        "\n",
        "      if candidate_value > maximum_value:\n",
        "        maximum_value = candidate_value\n",
        "        maximum_key = candidate_key\n",
        "\n",
        "  # print(f'maximum_key : {maximum_key}')\n",
        "  generated_text1.append(maximum_key)\n",
        "\n",
        "\n",
        "print(\" \".join(generated_text1))\n",
        "\n",
        "# for each in generated_text1:\n",
        "#   print(each , ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrKIJqofGEod",
        "outputId": "8fcb63d5-1672-47f9-b749-f404331123fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowing well the windings of the trail he halfwit scare countri locat unit state world cost almost restrict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementation of 5-Gram"
      ],
      "metadata": {
        "id": "hwgRQFGw6Lhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP_CA1/Tarzan.txt\"\n",
        "\n",
        "with open(data_path, 'r') as data_file:\n",
        "    text_data = data_file.read()\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# Remove special characters using regex\n",
        "text_data = re.sub(r'[^a-zA-Z0-9\\s]', '', text_data)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_data)\n",
        "\n",
        "# Remove Stop Words\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if not word in stops]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Create a dictionary to store the co-occurrence matrix for 5-gram\n",
        "co_occurrence_matrix_5 = {}\n",
        "\n",
        "all_fourth = []\n",
        "all_fifths = []\n",
        "\n",
        "# Construct the co-occurrence matrix for bigrams\n",
        "for i in range(len(tokens) - 4):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "    word3 = tokens[i + 2]\n",
        "    word4 = tokens[i + 3]\n",
        "    word5 = tokens[i + 4]\n",
        "\n",
        "    key_fourth  =  (word1 , word2 , word3 , word4)\n",
        "    key_fifths = (word1 , word2 , word3 , word4 , word5)\n",
        "\n",
        "    if key_fourth not in co_occurrence_matrix_5.keys():\n",
        "      co_occurrence_matrix_5[key_fourth] = 1\n",
        "      all_fourth.append(key_fourth)\n",
        "\n",
        "    if key_fifths not in co_occurrence_matrix_5.keys():\n",
        "      co_occurrence_matrix_5[key_fifths] = 1\n",
        "      all_fifths.append(key_fifths)\n",
        "\n",
        "    else :\n",
        "      co_occurrence_matrix_5[key_fourth] += 1\n",
        "      co_occurrence_matrix_5[key_fifths] += 1\n",
        "\n",
        "\n",
        "# for each in co_occurrence_matrix_3:\n",
        "#   if co_occurrence_matrix_3[each] == 0 :\n",
        "#     print(f'{each} : {co_occurrence_matrix_3[each]}')\n",
        "\n",
        "#   if 'got' in each or 'love' in each:\n",
        "#     print(f'{each} : {co_occurrence_matrix_3[each]}')\n",
        "\n",
        "print(f'all_fourth count  : {len(all_fourth)}')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "print('------------------------------------')\n",
        "print(f'all_fifths count  : {len(all_fifths)}')\n",
        "\n",
        "\n",
        "# THIS SECTION CALCULATE THE SEQUENCE PROBABILITY\n",
        "\n",
        "probability_5 = {}\n",
        "\n",
        "for i in range(len(tokens) - 4):\n",
        "    word1 = tokens[i]\n",
        "    word2 = tokens[i + 1]\n",
        "    word3 = tokens[i + 2]\n",
        "    word4 = tokens[i + 3]\n",
        "    word5 = tokens[i + 4]\n",
        "\n",
        "    key_fourth = (word1 , word2 , word3 , word4)\n",
        "    key_fifths = (word1 , word2 , word3 , word4 , word5)\n",
        "\n",
        "\n",
        "    probability_5[(word5 , key_fourth)] = co_occurrence_matrix_5[key_fifths] / co_occurrence_matrix_5[key_fourth]\n",
        "\n",
        "    if probability_5[(word5 , key_fourth)] != 1:\n",
        "        print(f'{ (word5 , key_fourth) } : {probability_5[(word5 , key_fourth)]}')\n",
        "\n",
        "    # if 'love' in key_fourth : # or 'love' in key_ternary:\n",
        "    #    print(f'{(word5 , key_fourth)} : {probability_5[(word5 , key_fourth)]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3riMmoo6PQo",
        "outputId": "c5b6a035-da11-4f6d-b1b1-0cc3c877b2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_pairs count  : 30863\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "all_terneris count  : 35981\n",
            "('pleas', ('full', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('avail', ('full', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('foundat', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('share', ('full', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('addit', ('state', 'check', 'law', 'countri')) : 0.5\n",
            "('term', ('full', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('1e6', ('term', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('specifi', ('full', 'project', 'gutenberg', 'licens')) : 0.5\n",
            "('royalti', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('address', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('provid', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('manag', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('owner', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('creat', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('effort', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('nonprofit', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('tax', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n",
            "('help', ('gutenberg', 'literari', 'archiv', 'foundat')) : 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test 5-Gram**"
      ],
      "metadata": {
        "id": "myDfSJlh6nfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text1 = ['Knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
        "\n",
        "last_words = (generated_text1[-4] , generated_text1[-3] , generated_text1[-2] , generated_text1[-1])\n",
        "maximum_value = 0\n",
        "maximum_key = 0\n",
        "\n",
        "for i in range(10):\n",
        "  maximum_value = 0\n",
        "  maximum_key = 0\n",
        "  last_words = (generated_text1[-4] , generated_text1[-3] , generated_text1[-2] , generated_text1[-1])\n",
        "\n",
        "  # print(f'last_words : {last_words}')\n",
        "  # print('------------------------------------')\n",
        "\n",
        "  for sequence in probability_5:\n",
        "    if last_words[3] in sequence[1]:\n",
        "      candidate_key = sequence[0]\n",
        "\n",
        "      # print(f'{sequence} : {probability_3[sequence]}')\n",
        "\n",
        "      if candidate_key in generated_text1[-5:]:\n",
        "        continue\n",
        "\n",
        "      candidate_value = probability_5[sequence]\n",
        "\n",
        "      if candidate_value > maximum_value:\n",
        "        maximum_value = candidate_value\n",
        "        maximum_key = candidate_key\n",
        "\n",
        "  print(f'maximum_key : {maximum_key}')\n",
        "  generated_text1.append(maximum_key)\n",
        "\n",
        "\n",
        "print(\" \".join(generated_text1))\n",
        "\n",
        "# for each in generated_text1:\n",
        "#   print(each , ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfOoLw5o6sGz",
        "outputId": "1a847c2e-f704-496b-f4a1-ab7495eda586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximum_key : halfwit\n",
            "maximum_key : scare\n",
            "maximum_key : countri\n",
            "maximum_key : locat\n",
            "maximum_key : unit\n",
            "maximum_key : state\n",
            "maximum_key : part\n",
            "maximum_key : world\n",
            "maximum_key : cost\n",
            "maximum_key : almost\n",
            "Knowing well the windings of the trail he halfwit scare countri locat unit state part world cost almost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using builtin libraries for N-Grams"
      ],
      "metadata": {
        "id": "8i9xNmC3511B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the generated text with the provided tokens\n",
        "generated_text1 = ['Knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
        "\n",
        "# Generate 3 more tokens to complete the sentence\n",
        "for _ in range(10):\n",
        "    # Get the last two tokens for the bigram prediction\n",
        "    context = tuple(generated_text1[-2:])\n",
        "\n",
        "    # Predict the next token using the bigram model\n",
        "    next_token = bigram_model.generate(text_seed=context)\n",
        "\n",
        "    # Append the predicted token to the generated text\n",
        "    generated_text1.append(next_token)\n",
        "\n",
        "# Print the complete generated text with 10 tokens\n",
        "print(' '.join(generated_text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-vhnyF33uOj",
        "outputId": "e915d278-26b6-4a36-8780-a5e47fa696a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowing well the windings of the trail he t h </s> a d i n e n g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the generated text with the provided tokens\n",
        "generated_text1 = ['For', 'half', 'a' , 'day' , 'he', 'lolled', 'on', 'the', 'huge', 'back' , 'and']\n",
        "\n",
        "# Generate 3 more tokens to complete the sentence\n",
        "for _ in range(10):\n",
        "    # Get the last two tokens for the bigram prediction\n",
        "    context = tuple(generated_text1[-2:])\n",
        "\n",
        "    # Predict the next token using the bigram model\n",
        "    next_token = bigram_model.generate(text_seed=context)\n",
        "\n",
        "    # Append the predicted token to the generated text\n",
        "    generated_text1.append(next_token)\n",
        "\n",
        "# Print the complete generated text with 10 tokens\n",
        "print(' '.join(generated_text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AuMlqYU4fgA",
        "outputId": "04c45eac-5892-45fb-a40f-52f983aa5ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For half a day he lolled on the huge back and </s> e e a r l o o n o\n"
          ]
        }
      ]
    }
  ]
}