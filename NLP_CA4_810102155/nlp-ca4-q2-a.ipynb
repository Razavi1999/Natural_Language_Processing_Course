{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import display, Javascript\n\ndef restart_kernel():\n    display(Javascript('IPython.notebook.kernel.restart()'))\n\n\nrestart_kernel()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft\n!pip install accelerate\n\n!pip install transformers\n!pip install datasets\n\n!pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers import HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\n\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()\n\n# 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","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"nyu-mll/multi_nli\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observing dataset to know what label is for which class","metadata":{}},{"cell_type":"code","source":"premises = dataset['train']['premise'][:10]\nhypothesis = dataset['train']['hypothesis'][:10]\nlabels = dataset['train']['label'][:10]\n\nfor item in range(10):\n    print('premise : ')\n    print(premises[item])\n    print('hypothesis : ')\n    print(hypothesis[item])\n    print('label : ')\n    print(labels[item])\n    print('-------------')\n\n# 0 entailment\n# 1 neural\n# 2 contradict\n# dataset['train'][:10]\n# dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero Shot Learning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n\ndataset = load_dataset(\"nyu-mll/multi_nli\")\n\nmodel_name = 'meta-llama/Meta-Llama-3-8B'\ntokenizer = AutoTokenizer.from_pretrained(model_name , force_download=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name , force_download=False)\n\n# 0 entailment\n# 1 neural\n# 2 contradict\n\ndef format_prompt(premise, hypothesis):\n    # From Jurafski book.\n    \n    description = f'A relationship of contradicts means that the premise contradicts the hypothesis\\\n    Entails means that the premise entails the hypothesis \\\n    neutral means that neither is necessarily true. \\ \n    Premise is : {premise} . \\n Hypothesis : {hypothesis}.\\\n    if Premise and Hypothesis have entailment relation return 0 . \\\n    else if Premise and Hypothesis have contradict relation return 2.\\\n    else if Premise and Hypothesis have neural relation return  1. \\ \n    Thank you so much.\n    '\n    return description\n\n\ndef preprocess_function(examples):\n    inputs = [format_prompt(premise, hypothesis)\n              for premise, hypothesis in\n              zip(examples['premise'], examples['hypothesis'])]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = examples[\"label\"]\n    return model_inputs\n\n\nencoded_dataset = dataset[:5].map(preprocess_function, batched=True)\n\n\nlabels_list = encoded_dataset['train']['labels']\nprint('labels : ')\nprint(labels_list)\n\ndef get_predictions(encoded_dataset):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for i in range(len(encoded_dataset['train'])):\n            input_ids = encoded_dataset['train']['input_ids'][i].unsqueeze(0)\n            attention_mask = encoded_dataset['train']['attention_mask'][i].unsqueeze(0)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            predicted_label = torch.argmax(logits, dim=-1).item()\n            predictions.append(predicted_label)\n    \n    print('predictions : ')\n    for item in predictions:\n        print(item)\n    print('--------------------------------')\n    \n    return predictions\n\nget_predictions(encoded_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One Shot Learning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"nyu-mll/multi_nli\")\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n\n# Example demonstration\ndemonstration = \"Premise: The person is at the park.\\nHypothesis: The person is outdoors.\\nLabel: entailment\\n\\n\"\n\ndef format_prompt(premise, hypothesis):\n    return f\"{demonstration}Premise: {premise}\\nHypothesis: {hypothesis}\\nLabel:\"\n\n\ndef preprocess_function(examples):\n    inputs = [format_prompt(premise, hypothesis) for premise, hypothesis in zip(examples['premise'], examples['hypothesis'])]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = tokenizer(examples[\"label\"], max_length=512, truncation=True, padding=\"max_length\")[\"input_ids\"]\n    return model_inputs\n\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    save_steps=1000,\n    temperature=0.7,  \n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation_matched\"],  # Using matched set for validation\n    tokenizer=tokenizer,\n)\n\n\ntrainer.train()\n\nmodel.save_pretrained(\"./fine_tuned_llama_3_8b\")\ntokenizer.save_pretrained(\"./fine_tuned_llama_3_8b\")\n\n\nresults = trainer.evaluate()\nprint(results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}